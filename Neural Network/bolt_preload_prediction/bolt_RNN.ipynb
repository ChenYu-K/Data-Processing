{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "import cv2 as cv\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlabel(dir):\n",
    "    labels=[]\n",
    "    for name in os.listdir(dir):\n",
    "        if os.path.splitext(name)[1] == '.png':\n",
    "            fname=os.path.splitext(name)[0]\n",
    "            labels.append(fname)\n",
    "    labels = list(map(int, labels))\n",
    "    labels = torch.tensor(labels)\n",
    "    # Normalize labels\n",
    "    #min = torch.min(labels)\n",
    "    #max = torch.max(labels)\n",
    "    #labels = (labels - min) / (max - min)\n",
    "    labels = labels.type(torch.FloatTensor)\n",
    "    return labels\n",
    "\n",
    "def generate_dataset(dir):\n",
    "    \"\"\"\n",
    "    set_label should be 'torch.tensor([1])' if two-catogory and positive sample\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    for file_name in os.listdir(dir):\n",
    "        img_dir = os.path.join(dir, file_name)\n",
    "        img = cv.imread(img_dir)\n",
    "        img = cv.resize(img, (769, 432))   # /5 resize img\n",
    "        #img_gray = cv.cvtColor(img,cv.COLOR_RGB2GRAY)\n",
    "        pimg = Image.fromarray(img)\n",
    "        train_data.append(pimg)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindir= './train/'\n",
    "validdir = './test/'\n",
    "train_data0 = generate_dataset(traindir)\n",
    "train_label0=getlabel(traindir)\n",
    "valid_data0 = generate_dataset(validdir)\n",
    "valid_label0 = getlabel(validdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.,   1.,  10., 100., 101., 102., 103., 104., 105., 106., 107., 108.,\n",
       "        109.,  11., 110., 111., 112., 113., 114., 115., 116., 117., 118., 119.,\n",
       "         12., 120., 122., 123., 124., 125., 126., 129.,  13., 130., 132., 133.,\n",
       "        134., 135., 136., 137., 138., 139.,  14., 140., 141., 142., 143., 144.,\n",
       "        145., 146., 147., 148., 149.,  15., 150., 151., 152., 153., 154., 155.,\n",
       "        156., 157., 159.,  16., 160., 162., 163., 164., 165., 166., 167., 168.,\n",
       "        169.,  17., 170., 171., 172., 173., 174., 175., 176., 177., 178., 179.,\n",
       "         18., 180., 182., 183., 184., 185., 186., 189.,  19., 190., 191., 192.,\n",
       "        193., 194., 195., 196., 197., 198., 199.,   2.,  20., 200., 201., 203.,\n",
       "        204., 205.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,   3.,\n",
       "         30.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,   4.,  43.,  44.,\n",
       "         45.,  46.,  47.,  48.,  49.,   5.,  50.,  51.,  52.,  53.,  54.,  55.,\n",
       "         56.,  57.,  58.,   6.,  60.,  61.,  62.,  64.,  65.,  67.,  68.,  69.,\n",
       "          7.,  70.,  71.,  74.,  75.,  76.,  77.,  78.,  79.,   8.,  80.,  81.,\n",
       "         82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,   9.,  90.,  91.,  92.,\n",
       "         93.,  94.,  95.,  96.,  97.,  98.,  99.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重写dataset类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        imgs = []\n",
    "        for i in range(len(labels)):\n",
    "            # print(type(data[i]))    # <class 'PIL.Image.Image'>\n",
    "            im_tensor = transform(data[i])#.to(torch.device(\"cpu\"))\n",
    "            imgs.append((im_tensor, labels[i]))\n",
    "        self.imgs = imgs                         # DataLoader通过getitem读取图片数据\n",
    "    def __getitem__(self, index):\n",
    "        fn, label = self.imgs[index]\n",
    "        return fn, label\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用MyDataset构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 432, 769])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "normMean = [0.5, 0.5, 0.5]\n",
    "normStd = [0.5, 0.5, 0.5]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(num_output_channels=1), #彩色图像转灰度图像num_output_channels默认1\n",
    "    transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]   \n",
    "    #transforms.Normalize(normMean, normStd)\n",
    "    ])\n",
    "\n",
    "# 也可以再定义train_transform加入一些数据增强 \n",
    "train_data = MyDataset(train_data0, train_label0, transform=transform)\n",
    "valid_data = MyDataset(valid_data0, valid_label0, transform=transform)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=1, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_data, batch_size=1, shuffle=True)\n",
    "dataiter=iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    # Net类的初始化函数\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        # 继承父类的初始化函数\n",
    "        super(Net, self).__init__()\n",
    "        # 网络的隐藏层创建，名称可以随便起\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(6)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(6*108*192, n_feature)\n",
    "        self.hidden_layer = torch.nn.Linear(n_feature, n_hidden)\n",
    "        # 输出层(预测层)创建，接收来自隐含层的数据\n",
    "        self.predict_layer = torch.nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    # 网络的前向传播函数，构造计算图\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))      #432-3+1=430,430/2=215;769-3+1=767,767/2=384\n",
    "        x = F.relu(self.bn2(self.conv2(x)))                #215-3+1=213,213/2=108;384-3+1=382,382/2=192\n",
    "        x = self.pool(x)   \n",
    "        x = x.view(-1, 6*108*192)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 用relu函数处理隐含层输出的结果并传给输出层\n",
    "        hidden_result = self.hidden_layer(x)\n",
    "        relu_result = F.relu(hidden_result)\n",
    "        predict_result = self.predict_layer(relu_result)\n",
    "        return predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAccuracy():\n",
    "    \n",
    "    net.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # run the model on the test set to predict labels\n",
    "            outputs = net(images)\n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "    \n",
    "    # compute the accuracy over all test images\n",
    "    accuracy = (100 * accuracy / total)\n",
    "    return(accuracy)\n",
    "\n",
    "def train(num_epochs):\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Define your execution device\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    # Convert model parameters and buffers to CPU or Cuda\n",
    "    net.to(device)\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader, 0):\n",
    "            #images, labels = dataiter.next()\n",
    "            # 输入数据进行预测\n",
    "            # get the inputs\n",
    "            images = Variable(images.to(device))\n",
    "            labels = Variable(labels.to(device))\n",
    "            prediction = net(images)\n",
    "            print(prediction,labels)\n",
    "            # 计算预测值与真值误差，注意参数顺序问题\n",
    "            # 第一个参数为预测值，第二个为真值\n",
    "            loss = loss_func(prediction, labels)\n",
    "\n",
    "            # 开始优化步骤\n",
    "            # 每次开始优化前将梯度置为0\n",
    "            optimizer.zero_grad()\n",
    "            # 误差反向传播\n",
    "            loss.backward()\n",
    "            # 按照最小loss优化参数\n",
    "            optimizer.step()\n",
    "            print('epoch:',i,'loss:',loss.item())\n",
    "            if i % 1000 == 999:    \n",
    "                # print every 1000 (twice per epoch) \n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 1000))\n",
    "                # zero the loss\n",
    "                running_loss = 0.0\n",
    "        # 计算准确率\n",
    "        # Compute and print the average accuracy fo this epoch when tested over all 10000 test images\n",
    "        accuracy = testAccuracy()\n",
    "        print('For epoch', epoch+1,'the test accuracy over the whole test set is %d %%' % (accuracy))\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "        #可视化训练结果\n",
    "        # if i % 2 == 0:\n",
    "        #     # 清空上一次显示结果\n",
    "        #     plt.cla()\n",
    "        #     # 无误差真值曲线\n",
    "        #     plt.plot(images.numpy(), labels.numpy(), c='blue', lw='3')\n",
    "        #     # 有误差散点\n",
    "        #     plt.scatter(images.numpy(), labels.numpy(), c='orange')\n",
    "        #     # 实时预测的曲线\n",
    "        #     plt.plot(images.numpy(), prediction.data.numpy(), c='red', lw='2')\n",
    "        #     plt.text(-0.5, -65, 'Time=%d Loss=%.4f' % (i, loss.data.numpy()), fontdict={'size': 15, 'color': 'red'})\n",
    "        #     plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 432, 769])\n",
      "torch.Size([1])\n",
      "tensor([72.])\n"
     ]
    }
   ],
   "source": [
    "Vdataiter=iter(valid_loader)\n",
    "vimg, vlabels = Vdataiter.next()\n",
    "print(vimg.shape)\n",
    "print(vlabels.shape)\n",
    "print(vlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, device):\n",
    "    Vdataiter=iter(valid_loader)\n",
    "    vimg, vlabels = Vdataiter.next()\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        vimg=vimg.to(device)\n",
    "        out = model(vimg)\n",
    "        #_, pre = torch.max(out.data, 1)\n",
    "        return vimg,out, vlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=124416, out_features=1000, bias=True)\n",
      "  (hidden_layer): Linear(in_features=1000, out_features=200, bias=True)\n",
      "  (predict_layer): Linear(in_features=200, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 训练次数\n",
    "TRAIN_TIMES = len(train_loader)-1\n",
    "# 输入输出的数据维度，这里都是1维\n",
    "INPUT_FEATURE_DIM = 1000\n",
    "OUTPUT_FEATURE_DIM = 1\n",
    "# 隐含层中神经元的个数\n",
    "NEURON_NUM = 200\n",
    "# 学习率，越大学的越快，但也容易造成不稳定，准确率上下波动的情况\n",
    "LEARNING_RATE = 0.1\n",
    "# unsqueeze函数可以将一维数据变成二维数据，在torch中只能处理二维数据\n",
    "#x_data = torch.unsqueeze(torch.linspace(-4, 4, 80), dim=1)\n",
    "# randn函数用于生成服从正态分布的随机数\n",
    "#y_data = x_data.pow(3) + 3 * torch.randn(x_data.size())\n",
    "#y_data_real = x_data.pow(3)\n",
    "\n",
    "# 定义模型\n",
    "net = Net(n_feature=INPUT_FEATURE_DIM, n_hidden=NEURON_NUM, n_output=OUTPUT_FEATURE_DIM)\n",
    "print(net)\n",
    "# 训练网络\n",
    "# 这里也可以使用其它的优化方法\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "# 定义一个误差计算方法\n",
    "loss_func = torch.nn.MSELoss() # 定义交叉熵损失函数 交叉熵损失函数是用来衡量两个概率分布之间的距离的#nn.MSELoss()\n",
    "# Define the loss function with Classification Cross-Entropy loss and an optimizer with Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0053]], grad_fn=<AddmmBackward0>) tensor([140.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brcy\\anaconda3\\envs\\pytorch-cy1\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 19598.525390625\n",
      "tensor([[735849.8750]], grad_fn=<AddmmBackward0>) tensor([43.])\n",
      "epoch: 1 loss: 541411770368.0\n",
      "tensor([[-35180.9805]], grad_fn=<AddmmBackward0>) tensor([169.])\n",
      "epoch: 2 loss: 1249621120.0\n",
      "tensor([[159564.5000]], grad_fn=<AddmmBackward0>) tensor([175.])\n",
      "epoch: 3 loss: 25405011968.0\n",
      "tensor([[-4647.2036]], grad_fn=<AddmmBackward0>) tensor([185.])\n",
      "epoch: 4 loss: 23350192.0\n",
      "tensor([[-628.1287]], grad_fn=<AddmmBackward0>) tensor([33.])\n",
      "epoch: 5 loss: 437091.1875\n",
      "tensor([[7590.9937]], grad_fn=<AddmmBackward0>) tensor([54.])\n",
      "epoch: 6 loss: 56806272.0\n",
      "tensor([[-2259.6536]], grad_fn=<AddmmBackward0>) tensor([48.])\n",
      "epoch: 7 loss: 5325265.0\n",
      "tensor([[-375.0234]], grad_fn=<AddmmBackward0>) tensor([65.])\n",
      "epoch: 8 loss: 193620.578125\n",
      "tensor([[1202.5931]], grad_fn=<AddmmBackward0>) tensor([45.])\n",
      "epoch: 9 loss: 1340021.875\n",
      "tensor([[-95.9383]], grad_fn=<AddmmBackward0>) tensor([198.])\n",
      "epoch: 10 loss: 86399.71875\n",
      "tensor([[722.3667]], grad_fn=<AddmmBackward0>) tensor([16.])\n",
      "epoch: 11 loss: 498953.90625\n",
      "tensor([[-1180.6362]], grad_fn=<AddmmBackward0>) tensor([39.])\n",
      "epoch: 12 loss: 1487512.5\n",
      "tensor([[-135.5691]], grad_fn=<AddmmBackward0>) tensor([58.])\n",
      "epoch: 13 loss: 37468.9921875\n",
      "tensor([[1225.3815]], grad_fn=<AddmmBackward0>) tensor([149.])\n",
      "epoch: 14 loss: 1158597.125\n",
      "tensor([[-953.3575]], grad_fn=<AddmmBackward0>) tensor([136.])\n",
      "epoch: 15 loss: 1186699.625\n",
      "tensor([[-321.5804]], grad_fn=<AddmmBackward0>) tensor([124.])\n",
      "epoch: 16 loss: 198541.84375\n",
      "tensor([[-136.7655]], grad_fn=<AddmmBackward0>) tensor([165.])\n",
      "epoch: 17 loss: 91062.3984375\n",
      "tensor([[500.2504]], grad_fn=<AddmmBackward0>) tensor([162.])\n",
      "epoch: 18 loss: 114413.3125\n",
      "tensor([[211.2749]], grad_fn=<AddmmBackward0>) tensor([189.])\n",
      "epoch: 19 loss: 496.1719665527344\n",
      "tensor([[-91.3917]], grad_fn=<AddmmBackward0>) tensor([190.])\n",
      "epoch: 20 loss: 79181.265625\n",
      "tensor([[40.6150]], grad_fn=<AddmmBackward0>) tensor([88.])\n",
      "epoch: 21 loss: 2245.335205078125\n",
      "tensor([[310.7231]], grad_fn=<AddmmBackward0>) tensor([81.])\n",
      "epoch: 22 loss: 52772.6953125\n",
      "tensor([[9.2725]], grad_fn=<AddmmBackward0>) tensor([53.])\n",
      "epoch: 23 loss: 1912.09765625\n",
      "tensor([[110.3338]], grad_fn=<AddmmBackward0>) tensor([109.])\n",
      "epoch: 24 loss: 1.7790868282318115\n",
      "tensor([[-111.2305]], grad_fn=<AddmmBackward0>) tensor([22.])\n",
      "epoch: 25 loss: 17750.365234375\n",
      "tensor([[-16.4368]], grad_fn=<AddmmBackward0>) tensor([29.])\n",
      "epoch: 26 loss: 2064.49951171875\n",
      "tensor([[52.7737]], grad_fn=<AddmmBackward0>) tensor([193.])\n",
      "epoch: 27 loss: 19663.40234375\n",
      "tensor([[398.6592]], grad_fn=<AddmmBackward0>) tensor([36.])\n",
      "epoch: 28 loss: 131521.6875\n",
      "tensor([[184.2371]], grad_fn=<AddmmBackward0>) tensor([17.])\n",
      "epoch: 29 loss: 27968.25\n",
      "tensor([[12.8396]], grad_fn=<AddmmBackward0>) tensor([139.])\n",
      "epoch: 30 loss: 15916.4404296875\n",
      "tensor([[-1.2405]], grad_fn=<AddmmBackward0>) tensor([192.])\n",
      "epoch: 31 loss: 37341.875\n",
      "tensor([[14.9023]], grad_fn=<AddmmBackward0>) tensor([201.])\n",
      "epoch: 32 loss: 34632.35546875\n",
      "tensor([[673.3347]], grad_fn=<AddmmBackward0>) tensor([62.])\n",
      "epoch: 33 loss: 373730.0625\n",
      "tensor([[-32.9499]], grad_fn=<AddmmBackward0>) tensor([156.])\n",
      "epoch: 34 loss: 35702.05078125\n",
      "tensor([[-17.1928]], grad_fn=<AddmmBackward0>) tensor([108.])\n",
      "epoch: 35 loss: 15673.234375\n",
      "tensor([[-1.2582]], grad_fn=<AddmmBackward0>) tensor([46.])\n",
      "epoch: 36 loss: 2233.335205078125\n",
      "tensor([[-1.2593]], grad_fn=<AddmmBackward0>) tensor([74.])\n",
      "epoch: 37 loss: 5663.966796875\n",
      "tensor([[-1.2580]], grad_fn=<AddmmBackward0>) tensor([170.])\n",
      "epoch: 38 loss: 29329.291015625\n",
      "tensor([[-62.6824]], grad_fn=<AddmmBackward0>) tensor([38.])\n",
      "epoch: 39 loss: 10136.9462890625\n",
      "tensor([[27.8440]], grad_fn=<AddmmBackward0>) tensor([95.])\n",
      "epoch: 40 loss: 4509.923828125\n",
      "tensor([[245.5110]], grad_fn=<AddmmBackward0>) tensor([178.])\n",
      "epoch: 41 loss: 4557.7392578125\n",
      "tensor([[46.2887]], grad_fn=<AddmmBackward0>) tensor([205.])\n",
      "epoch: 42 loss: 25189.2578125\n",
      "tensor([[394.7638]], grad_fn=<AddmmBackward0>) tensor([151.])\n",
      "epoch: 43 loss: 59420.7734375\n",
      "tensor([[108.0167]], grad_fn=<AddmmBackward0>) tensor([160.])\n",
      "epoch: 44 loss: 2702.267333984375\n",
      "tensor([[-1.2045]], grad_fn=<AddmmBackward0>) tensor([18.])\n",
      "epoch: 45 loss: 368.8126220703125\n",
      "tensor([[-1.2002]], grad_fn=<AddmmBackward0>) tensor([130.])\n",
      "epoch: 46 loss: 17213.478515625\n",
      "tensor([[-1.1915]], grad_fn=<AddmmBackward0>) tensor([163.])\n",
      "epoch: 47 loss: 26958.84375\n",
      "tensor([[-1.1776]], grad_fn=<AddmmBackward0>) tensor([13.])\n",
      "epoch: 48 loss: 201.00527954101562\n",
      "tensor([[-1.1645]], grad_fn=<AddmmBackward0>) tensor([89.])\n",
      "epoch: 49 loss: 8129.64208984375\n",
      "tensor([[-1.1493]], grad_fn=<AddmmBackward0>) tensor([14.])\n",
      "epoch: 50 loss: 229.50035095214844\n",
      "tensor([[-1.1348]], grad_fn=<AddmmBackward0>) tensor([155.])\n",
      "epoch: 51 loss: 24378.083984375\n",
      "tensor([[-1.1158]], grad_fn=<AddmmBackward0>) tensor([64.])\n",
      "epoch: 52 loss: 4240.0732421875\n",
      "tensor([[-1.0961]], grad_fn=<AddmmBackward0>) tensor([105.])\n",
      "epoch: 53 loss: 11256.3828125\n",
      "tensor([[-1.0741]], grad_fn=<AddmmBackward0>) tensor([34.])\n",
      "epoch: 54 loss: 1230.1927490234375\n",
      "tensor([[-1.0528]], grad_fn=<AddmmBackward0>) tensor([153.])\n",
      "epoch: 55 loss: 23732.25\n",
      "tensor([[-1.0274]], grad_fn=<AddmmBackward0>) tensor([143.])\n",
      "epoch: 56 loss: 20743.888671875\n",
      "tensor([[-0.9987]], grad_fn=<AddmmBackward0>) tensor([4.])\n",
      "epoch: 57 loss: 24.987327575683594\n",
      "tensor([[-0.9725]], grad_fn=<AddmmBackward0>) tensor([20.])\n",
      "epoch: 58 loss: 439.8461608886719\n",
      "tensor([[-0.9479]], grad_fn=<AddmmBackward0>) tensor([76.])\n",
      "epoch: 59 loss: 5920.97216796875\n",
      "tensor([[-0.9224]], grad_fn=<AddmmBackward0>) tensor([199.])\n",
      "epoch: 60 loss: 39968.95703125\n",
      "tensor([[-0.8912]], grad_fn=<AddmmBackward0>) tensor([191.])\n",
      "epoch: 61 loss: 36822.25390625\n",
      "tensor([[-0.8553]], grad_fn=<AddmmBackward0>) tensor([111.])\n",
      "epoch: 62 loss: 12511.6083984375\n",
      "tensor([[-0.8181]], grad_fn=<AddmmBackward0>) tensor([23.])\n",
      "epoch: 63 loss: 567.303955078125\n",
      "tensor([[-0.7834]], grad_fn=<AddmmBackward0>) tensor([168.])\n",
      "epoch: 64 loss: 28487.84765625\n",
      "tensor([[-0.7450]], grad_fn=<AddmmBackward0>) tensor([28.])\n",
      "epoch: 65 loss: 826.2770385742188\n",
      "tensor([[-0.7090]], grad_fn=<AddmmBackward0>) tensor([79.])\n",
      "epoch: 66 loss: 6353.5234375\n",
      "tensor([[-0.6730]], grad_fn=<AddmmBackward0>) tensor([6.])\n",
      "epoch: 67 loss: 44.528385162353516\n",
      "tensor([[-0.6400]], grad_fn=<AddmmBackward0>) tensor([146.])\n",
      "epoch: 68 loss: 21503.28515625\n",
      "tensor([[-0.6039]], grad_fn=<AddmmBackward0>) tensor([103.])\n",
      "epoch: 69 loss: 10733.765625\n",
      "tensor([[-0.5667]], grad_fn=<AddmmBackward0>) tensor([12.])\n",
      "epoch: 70 loss: 157.92288208007812\n",
      "tensor([[-0.5328]], grad_fn=<AddmmBackward0>) tensor([68.])\n",
      "epoch: 71 loss: 4696.7412109375\n",
      "tensor([[-0.4991]], grad_fn=<AddmmBackward0>) tensor([119.])\n",
      "epoch: 72 loss: 14280.033203125\n",
      "tensor([[-0.4634]], grad_fn=<AddmmBackward0>) tensor([44.])\n",
      "epoch: 73 loss: 1976.99609375\n",
      "tensor([[-0.4291]], grad_fn=<AddmmBackward0>) tensor([11.])\n",
      "epoch: 74 loss: 130.6254425048828\n",
      "tensor([[-0.3976]], grad_fn=<AddmmBackward0>) tensor([135.])\n",
      "epoch: 75 loss: 18332.505859375\n",
      "tensor([[-0.3630]], grad_fn=<AddmmBackward0>) tensor([173.])\n",
      "epoch: 76 loss: 30054.7421875\n",
      "tensor([[-0.3241]], grad_fn=<AddmmBackward0>) tensor([171.])\n",
      "epoch: 77 loss: 29351.95703125\n",
      "tensor([[-0.2814]], grad_fn=<AddmmBackward0>) tensor([27.])\n",
      "epoch: 78 loss: 744.2733764648438\n",
      "tensor([[-0.2417]], grad_fn=<AddmmBackward0>) tensor([180.])\n",
      "epoch: 79 loss: 32487.076171875\n",
      "tensor([[-0.1979]], grad_fn=<AddmmBackward0>) tensor([197.])\n",
      "epoch: 80 loss: 38887.00390625\n",
      "tensor([[-0.1495]], grad_fn=<AddmmBackward0>) tensor([176.])\n",
      "epoch: 81 loss: 31028.654296875\n",
      "tensor([[-0.0980]], grad_fn=<AddmmBackward0>) tensor([77.])\n",
      "epoch: 82 loss: 5944.0966796875\n",
      "tensor([[-0.0478]], grad_fn=<AddmmBackward0>) tensor([107.])\n",
      "epoch: 83 loss: 11459.232421875\n",
      "tensor([[0.0024]], grad_fn=<AddmmBackward0>) tensor([102.])\n",
      "epoch: 84 loss: 10403.501953125\n",
      "tensor([[0.0526]], grad_fn=<AddmmBackward0>) tensor([86.])\n",
      "epoch: 85 loss: 7386.96240234375\n",
      "tensor([[0.1019]], grad_fn=<AddmmBackward0>) tensor([122.])\n",
      "epoch: 86 loss: 14859.15234375\n",
      "tensor([[0.1521]], grad_fn=<AddmmBackward0>) tensor([47.])\n",
      "epoch: 87 loss: 2194.7265625\n",
      "tensor([[0.1997]], grad_fn=<AddmmBackward0>) tensor([150.])\n",
      "epoch: 88 loss: 22440.11328125\n",
      "tensor([[0.2498]], grad_fn=<AddmmBackward0>) tensor([116.])\n",
      "epoch: 89 loss: 13398.1123046875\n",
      "tensor([[0.3005]], grad_fn=<AddmmBackward0>) tensor([166.])\n",
      "epoch: 90 loss: 27456.33203125\n",
      "tensor([[0.3540]], grad_fn=<AddmmBackward0>) tensor([113.])\n",
      "epoch: 91 loss: 12689.1171875\n",
      "tensor([[0.4078]], grad_fn=<AddmmBackward0>) tensor([120.])\n",
      "epoch: 92 loss: 14302.2978515625\n",
      "tensor([[0.4621]], grad_fn=<AddmmBackward0>) tensor([5.])\n",
      "epoch: 93 loss: 20.59275245666504\n",
      "tensor([[0.5115]], grad_fn=<AddmmBackward0>) tensor([21.])\n",
      "epoch: 94 loss: 419.7803649902344\n",
      "tensor([[0.5572]], grad_fn=<AddmmBackward0>) tensor([26.])\n",
      "epoch: 95 loss: 647.3383178710938\n",
      "tensor([[0.5998]], grad_fn=<AddmmBackward0>) tensor([50.])\n",
      "epoch: 96 loss: 2440.3837890625\n",
      "tensor([[0.6407]], grad_fn=<AddmmBackward0>) tensor([159.])\n",
      "epoch: 97 loss: 25077.658203125\n",
      "tensor([[0.6855]], grad_fn=<AddmmBackward0>) tensor([37.])\n",
      "epoch: 98 loss: 1318.7447509765625\n",
      "tensor([[-75.6290]], grad_fn=<AddmmBackward0>) tensor([51.])\n",
      "epoch: 99 loss: 16034.9033203125\n",
      "tensor([[0.8361]], grad_fn=<AddmmBackward0>) tensor([183.])\n",
      "epoch: 100 loss: 33183.69140625\n",
      "tensor([[485.7611]], grad_fn=<AddmmBackward0>) tensor([60.])\n",
      "epoch: 101 loss: 181272.546875\n",
      "tensor([[0.8343]], grad_fn=<AddmmBackward0>) tensor([112.])\n",
      "epoch: 102 loss: 12357.81640625\n",
      "tensor([[0.8487]], grad_fn=<AddmmBackward0>) tensor([195.])\n",
      "epoch: 103 loss: 37694.734375\n",
      "tensor([[0.8707]], grad_fn=<AddmmBackward0>) tensor([126.])\n",
      "epoch: 104 loss: 15657.3388671875\n",
      "tensor([[0.8963]], grad_fn=<AddmmBackward0>) tensor([184.])\n",
      "epoch: 105 loss: 33526.953125\n",
      "tensor([[0.9279]], grad_fn=<AddmmBackward0>) tensor([8.])\n",
      "epoch: 106 loss: 50.015140533447266\n",
      "tensor([[0.9565]], grad_fn=<AddmmBackward0>) tensor([138.])\n",
      "epoch: 107 loss: 18780.9296875\n",
      "tensor([[0.9886]], grad_fn=<AddmmBackward0>) tensor([49.])\n",
      "epoch: 108 loss: 2305.098876953125\n",
      "tensor([[1.0197]], grad_fn=<AddmmBackward0>) tensor([200.])\n",
      "epoch: 109 loss: 39593.1796875\n",
      "tensor([[1.0569]], grad_fn=<AddmmBackward0>) tensor([118.])\n",
      "epoch: 110 loss: 13675.697265625\n",
      "tensor([[1.0958]], grad_fn=<AddmmBackward0>) tensor([132.])\n",
      "epoch: 111 loss: 17135.90625\n",
      "tensor([[1.1370]], grad_fn=<AddmmBackward0>) tensor([96.])\n",
      "epoch: 112 loss: 8998.98828125\n",
      "tensor([[1.1786]], grad_fn=<AddmmBackward0>) tensor([92.])\n",
      "epoch: 113 loss: 8248.5341796875\n",
      "tensor([[1.2203]], grad_fn=<AddmmBackward0>) tensor([133.])\n",
      "epoch: 114 loss: 17365.892578125\n",
      "tensor([[1.2641]], grad_fn=<AddmmBackward0>) tensor([115.])\n",
      "epoch: 115 loss: 12935.8564453125\n",
      "tensor([[1.3090]], grad_fn=<AddmmBackward0>) tensor([147.])\n",
      "epoch: 116 loss: 21225.87890625\n",
      "tensor([[1.3563]], grad_fn=<AddmmBackward0>) tensor([70.])\n",
      "epoch: 117 loss: 4711.95751953125\n",
      "tensor([[1.4023]], grad_fn=<AddmmBackward0>) tensor([83.])\n",
      "epoch: 118 loss: 6658.18603515625\n",
      "tensor([[1.4477]], grad_fn=<AddmmBackward0>) tensor([145.])\n",
      "epoch: 119 loss: 20607.265625\n",
      "tensor([[1.4955]], grad_fn=<AddmmBackward0>) tensor([106.])\n",
      "epoch: 120 loss: 10921.1845703125\n",
      "tensor([[1.5437]], grad_fn=<AddmmBackward0>) tensor([203.])\n",
      "epoch: 121 loss: 40584.62890625\n",
      "tensor([[1.5968]], grad_fn=<AddmmBackward0>) tensor([144.])\n",
      "epoch: 122 loss: 20278.658203125\n",
      "tensor([[1.6517]], grad_fn=<AddmmBackward0>) tensor([10.])\n",
      "epoch: 123 loss: 69.69479370117188\n",
      "tensor([[1.7016]], grad_fn=<AddmmBackward0>) tensor([15.])\n",
      "epoch: 124 loss: 176.8473663330078\n",
      "tensor([[1.7474]], grad_fn=<AddmmBackward0>) tensor([7.])\n",
      "epoch: 125 loss: 27.589914321899414\n",
      "tensor([[1.7890]], grad_fn=<AddmmBackward0>) tensor([101.])\n",
      "epoch: 126 loss: 9842.8173828125\n",
      "tensor([[1.8315]], grad_fn=<AddmmBackward0>) tensor([110.])\n",
      "epoch: 127 loss: 11700.4169921875\n",
      "tensor([[1.8753]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "epoch: 128 loss: 0.7661512494087219\n",
      "tensor([[1.9148]], grad_fn=<AddmmBackward0>) tensor([82.])\n",
      "epoch: 129 loss: 6413.6376953125\n",
      "tensor([[1.9545]], grad_fn=<AddmmBackward0>) tensor([134.])\n",
      "epoch: 130 loss: 17436.005859375\n",
      "tensor([[1.9970]], grad_fn=<AddmmBackward0>) tensor([87.])\n",
      "epoch: 131 loss: 7225.501953125\n",
      "tensor([[2.0398]], grad_fn=<AddmmBackward0>) tensor([182.])\n",
      "epoch: 132 loss: 32385.685546875\n",
      "tensor([[2.0874]], grad_fn=<AddmmBackward0>) tensor([98.])\n",
      "epoch: 133 loss: 9199.220703125\n",
      "tensor([[2.1354]], grad_fn=<AddmmBackward0>) tensor([125.])\n",
      "epoch: 134 loss: 15095.7099609375\n",
      "tensor([[2.1850]], grad_fn=<AddmmBackward0>) tensor([78.])\n",
      "epoch: 135 loss: 5747.90869140625\n",
      "tensor([[2.2338]], grad_fn=<AddmmBackward0>) tensor([32.])\n",
      "epoch: 136 loss: 886.0260620117188\n",
      "tensor([[2.2795]], grad_fn=<AddmmBackward0>) tensor([52.])\n",
      "epoch: 137 loss: 2472.131591796875\n",
      "tensor([[2.3233]], grad_fn=<AddmmBackward0>) tensor([90.])\n",
      "epoch: 138 loss: 7687.1962890625\n",
      "tensor([[2.3676]], grad_fn=<AddmmBackward0>) tensor([157.])\n",
      "epoch: 139 loss: 23911.1796875\n",
      "tensor([[2.4157]], grad_fn=<AddmmBackward0>) tensor([91.])\n",
      "epoch: 140 loss: 7847.17919921875\n",
      "tensor([[2.4639]], grad_fn=<AddmmBackward0>) tensor([137.])\n",
      "epoch: 141 loss: 18099.970703125\n",
      "tensor([[2.5145]], grad_fn=<AddmmBackward0>) tensor([24.])\n",
      "epoch: 142 loss: 461.6250915527344\n",
      "tensor([[2.5615]], grad_fn=<AddmmBackward0>) tensor([186.])\n",
      "epoch: 143 loss: 33649.67578125\n",
      "tensor([[2.6138]], grad_fn=<AddmmBackward0>) tensor([141.])\n",
      "epoch: 144 loss: 19150.75390625\n",
      "tensor([[2.6684]], grad_fn=<AddmmBackward0>) tensor([25.])\n",
      "epoch: 145 loss: 498.6996765136719\n",
      "tensor([[2.7191]], grad_fn=<AddmmBackward0>) tensor([19.])\n",
      "epoch: 146 loss: 265.068603515625\n",
      "tensor([[2.7658]], grad_fn=<AddmmBackward0>) tensor([71.])\n",
      "epoch: 147 loss: 4655.9091796875\n",
      "tensor([[2.8118]], grad_fn=<AddmmBackward0>) tensor([84.])\n",
      "epoch: 148 loss: 6591.529296875\n",
      "tensor([[2.8578]], grad_fn=<AddmmBackward0>) tensor([104.])\n",
      "epoch: 149 loss: 10229.7373046875\n",
      "tensor([[2.9051]], grad_fn=<AddmmBackward0>) tensor([148.])\n",
      "epoch: 150 loss: 21052.533203125\n",
      "tensor([[2.9559]], grad_fn=<AddmmBackward0>) tensor([56.])\n",
      "epoch: 151 loss: 2813.677490234375\n",
      "tensor([[3.0048]], grad_fn=<AddmmBackward0>) tensor([100.])\n",
      "epoch: 152 loss: 9408.06640625\n",
      "tensor([[3.0545]], grad_fn=<AddmmBackward0>) tensor([69.])\n",
      "epoch: 153 loss: 4348.80615234375\n",
      "tensor([[3.1032]], grad_fn=<AddmmBackward0>) tensor([85.])\n",
      "epoch: 154 loss: 6707.0830078125\n",
      "tensor([[3.1519]], grad_fn=<AddmmBackward0>) tensor([194.])\n",
      "epoch: 155 loss: 36422.984375\n",
      "tensor([[3.2068]], grad_fn=<AddmmBackward0>) tensor([129.])\n",
      "epoch: 156 loss: 15823.9169921875\n",
      "tensor([[3.2637]], grad_fn=<AddmmBackward0>) tensor([30.])\n",
      "epoch: 157 loss: 714.827880859375\n",
      "tensor([[3.3167]], grad_fn=<AddmmBackward0>) tensor([179.])\n",
      "epoch: 158 loss: 30864.607421875\n",
      "tensor([[3.3749]], grad_fn=<AddmmBackward0>) tensor([55.])\n",
      "epoch: 159 loss: 2665.156005859375\n",
      "tensor([[3.4304]], grad_fn=<AddmmBackward0>) tensor([57.])\n",
      "epoch: 160 loss: 2869.697021484375\n",
      "tensor([[3.4839]], grad_fn=<AddmmBackward0>) tensor([154.])\n",
      "epoch: 161 loss: 22655.095703125\n",
      "tensor([[3.5411]], grad_fn=<AddmmBackward0>) tensor([123.])\n",
      "epoch: 162 loss: 14270.421875\n",
      "tensor([[3.6000]], grad_fn=<AddmmBackward0>) tensor([0.])\n",
      "epoch: 163 loss: 12.959968566894531\n",
      "tensor([[3.6530]], grad_fn=<AddmmBackward0>) tensor([177.])\n",
      "epoch: 164 loss: 30049.171875\n",
      "tensor([[3.7114]], grad_fn=<AddmmBackward0>) tensor([67.])\n",
      "epoch: 165 loss: 4005.448486328125\n",
      "tensor([[3.7680]], grad_fn=<AddmmBackward0>) tensor([94.])\n",
      "epoch: 166 loss: 8141.81005859375\n",
      "tensor([[3.8248]], grad_fn=<AddmmBackward0>) tensor([174.])\n",
      "epoch: 167 loss: 28959.609375\n",
      "tensor([[3.8865]], grad_fn=<AddmmBackward0>) tensor([2.])\n",
      "epoch: 168 loss: 3.558755874633789\n",
      "tensor([[3.9422]], grad_fn=<AddmmBackward0>) tensor([196.])\n",
      "epoch: 169 loss: 36886.2109375\n",
      "tensor([[4.0044]], grad_fn=<AddmmBackward0>) tensor([80.])\n",
      "epoch: 170 loss: 5775.33447265625\n",
      "tensor([[4.0654]], grad_fn=<AddmmBackward0>) tensor([9.])\n",
      "epoch: 171 loss: 24.35011100769043\n",
      "tensor([[4.1209]], grad_fn=<AddmmBackward0>) tensor([93.])\n",
      "epoch: 172 loss: 7899.4873046875\n",
      "tensor([[4.1768]], grad_fn=<AddmmBackward0>) tensor([172.])\n",
      "epoch: 173 loss: 28164.630859375\n",
      "tensor([[4.2379]], grad_fn=<AddmmBackward0>) tensor([114.])\n",
      "epoch: 174 loss: 12047.7138671875\n",
      "tensor([[4.3002]], grad_fn=<AddmmBackward0>) tensor([61.])\n",
      "epoch: 175 loss: 3214.8623046875\n",
      "tensor([[4.3603]], grad_fn=<AddmmBackward0>) tensor([167.])\n",
      "epoch: 176 loss: 26451.6796875\n",
      "tensor([[4.4251]], grad_fn=<AddmmBackward0>) tensor([152.])\n",
      "epoch: 177 loss: 21778.357421875\n",
      "tensor([[4.4933]], grad_fn=<AddmmBackward0>) tensor([164.])\n",
      "epoch: 178 loss: 25442.396484375\n",
      "tensor([[4.5654]], grad_fn=<AddmmBackward0>) tensor([35.])\n",
      "epoch: 179 loss: 926.2664184570312\n",
      "tensor([[4.6326]], grad_fn=<AddmmBackward0>) tensor([3.])\n",
      "epoch: 180 loss: 2.6654911041259766\n",
      "tensor([[4.6934]], grad_fn=<AddmmBackward0>) tensor([204.])\n",
      "epoch: 181 loss: 39723.1328125\n",
      "tensor([[4.7615]], grad_fn=<AddmmBackward0>) tensor([97.])\n",
      "epoch: 182 loss: 8507.9326171875\n",
      "tensor([[4.8294]], grad_fn=<AddmmBackward0>) tensor([99.])\n",
      "epoch: 183 loss: 8868.0966796875\n",
      "tensor([[4.8972]], grad_fn=<AddmmBackward0>) tensor([142.])\n",
      "epoch: 184 loss: 18797.169921875\n",
      "tensor([[4.9679]], grad_fn=<AddmmBackward0>) tensor([75.])\n",
      "epoch: 185 loss: 4904.49560546875\n",
      "tensor([[5.0367]], grad_fn=<AddmmBackward0>) tensor([117.])\n",
      "epoch: 186 loss: 12535.7900390625\n",
      "For epoch 1 the test accuracy over the whole test set is 0 %\n",
      "tensor([[5.1066]], grad_fn=<AddmmBackward0>) tensor([123.])\n",
      "epoch: 0 loss: 13898.8525390625\n",
      "tensor([[5.1781]], grad_fn=<AddmmBackward0>) tensor([122.])\n",
      "epoch: 1 loss: 13647.359375\n",
      "tensor([[5.2509]], grad_fn=<AddmmBackward0>) tensor([5.])\n",
      "epoch: 2 loss: 0.06297224014997482\n",
      "tensor([[5.3168]], grad_fn=<AddmmBackward0>) tensor([62.])\n",
      "epoch: 3 loss: 3212.979736328125\n",
      "tensor([[5.3805]], grad_fn=<AddmmBackward0>) tensor([101.])\n",
      "epoch: 4 loss: 9143.0859375\n",
      "tensor([[5.4450]], grad_fn=<AddmmBackward0>) tensor([159.])\n",
      "epoch: 5 loss: 23579.150390625\n",
      "tensor([[5.5143]], grad_fn=<AddmmBackward0>) tensor([25.])\n",
      "epoch: 6 loss: 379.6925964355469\n",
      "tensor([[5.5784]], grad_fn=<AddmmBackward0>) tensor([54.])\n",
      "epoch: 7 loss: 2344.64697265625\n",
      "tensor([[5.6399]], grad_fn=<AddmmBackward0>) tensor([199.])\n",
      "epoch: 8 loss: 37388.125\n",
      "tensor([[16.0184]], grad_fn=<AddmmBackward0>) tensor([26.])\n",
      "epoch: 9 loss: 99.6316909790039\n",
      "tensor([[3436.9116]], grad_fn=<AddmmBackward0>) tensor([4.])\n",
      "epoch: 10 loss: 11784882.0\n",
      "tensor([[144.6862]], grad_fn=<AddmmBackward0>) tensor([107.])\n",
      "epoch: 11 loss: 1420.2498779296875\n",
      "tensor([[-0.8885]], grad_fn=<AddmmBackward0>) tensor([185.])\n",
      "epoch: 12 loss: 34554.546875\n",
      "tensor([[-0.6052]], grad_fn=<AddmmBackward0>) tensor([170.])\n",
      "epoch: 13 loss: 29106.126953125\n",
      "tensor([[-2.2551]], grad_fn=<AddmmBackward0>) tensor([163.])\n",
      "epoch: 14 loss: 27309.2578125\n",
      "tensor([[-3.7136]], grad_fn=<AddmmBackward0>) tensor([89.])\n",
      "epoch: 15 loss: 8595.8134765625\n",
      "tensor([[-5.0043]], grad_fn=<AddmmBackward0>) tensor([77.])\n",
      "epoch: 16 loss: 6724.70556640625\n",
      "tensor([[-6.1470]], grad_fn=<AddmmBackward0>) tensor([197.])\n",
      "epoch: 17 loss: 41268.6875\n",
      "tensor([[-44.3628]], grad_fn=<AddmmBackward0>) tensor([154.])\n",
      "epoch: 18 loss: 39347.80859375\n",
      "tensor([[1130.5519]], grad_fn=<AddmmBackward0>) tensor([150.])\n",
      "epoch: 19 loss: 961482.0\n",
      "tensor([[-940.6519]], grad_fn=<AddmmBackward0>) tensor([13.])\n",
      "epoch: 20 loss: 909451.875\n",
      "tensor([[-9.5439]], grad_fn=<AddmmBackward0>) tensor([166.])\n",
      "epoch: 21 loss: 30815.671875\n",
      "tensor([[-10.1529]], grad_fn=<AddmmBackward0>) tensor([155.])\n",
      "epoch: 22 loss: 27275.46875\n",
      "tensor([[-10.6746]], grad_fn=<AddmmBackward0>) tensor([152.])\n",
      "epoch: 23 loss: 26463.01171875\n",
      "tensor([[-11.1157]], grad_fn=<AddmmBackward0>) tensor([36.])\n",
      "epoch: 24 loss: 2219.886962890625\n",
      "tensor([[-11.4984]], grad_fn=<AddmmBackward0>) tensor([47.])\n",
      "epoch: 25 loss: 3422.06201171875\n",
      "tensor([[-11.8273]], grad_fn=<AddmmBackward0>) tensor([11.])\n",
      "epoch: 26 loss: 521.0859375\n",
      "tensor([[-12.1137]], grad_fn=<AddmmBackward0>) tensor([2.])\n",
      "epoch: 27 loss: 199.1975555419922\n",
      "tensor([[-12.3642]], grad_fn=<AddmmBackward0>) tensor([104.])\n",
      "epoch: 28 loss: 13540.615234375\n",
      "tensor([[-12.5640]], grad_fn=<AddmmBackward0>) tensor([160.])\n",
      "epoch: 29 loss: 29778.34765625\n",
      "tensor([[50.1512]], grad_fn=<AddmmBackward0>) tensor([96.])\n",
      "epoch: 30 loss: 2102.109130859375\n",
      "tensor([[1050.1881]], grad_fn=<AddmmBackward0>) tensor([200.])\n",
      "epoch: 31 loss: 722819.8125\n",
      "tensor([[-438.8202]], grad_fn=<AddmmBackward0>) tensor([32.])\n",
      "epoch: 32 loss: 221671.6875\n",
      "tensor([[450.7353]], grad_fn=<AddmmBackward0>) tensor([29.])\n",
      "epoch: 33 loss: 177860.625\n",
      "tensor([[-13.1293]], grad_fn=<AddmmBackward0>) tensor([151.])\n",
      "epoch: 34 loss: 26938.41796875\n",
      "tensor([[3.8067]], grad_fn=<AddmmBackward0>) tensor([168.])\n",
      "epoch: 35 loss: 26959.455078125\n",
      "tensor([[26.3484]], grad_fn=<AddmmBackward0>) tensor([172.])\n",
      "epoch: 36 loss: 21214.38671875\n",
      "tensor([[371.9920]], grad_fn=<AddmmBackward0>) tensor([80.])\n",
      "epoch: 37 loss: 85259.3125\n",
      "tensor([[-390.7523]], grad_fn=<AddmmBackward0>) tensor([19.])\n",
      "epoch: 38 loss: 167896.9375\n",
      "tensor([[-43.6664]], grad_fn=<AddmmBackward0>) tensor([193.])\n",
      "epoch: 39 loss: 56010.9921875\n",
      "tensor([[35.1960]], grad_fn=<AddmmBackward0>) tensor([100.])\n",
      "epoch: 40 loss: 4199.5556640625\n",
      "tensor([[141.3131]], grad_fn=<AddmmBackward0>) tensor([139.])\n",
      "epoch: 41 loss: 5.350550174713135\n",
      "tensor([[-309.1054]], grad_fn=<AddmmBackward0>) tensor([130.])\n",
      "epoch: 42 loss: 192813.5625\n",
      "tensor([[282.8817]], grad_fn=<AddmmBackward0>) tensor([94.])\n",
      "epoch: 43 loss: 35676.27734375\n",
      "tensor([[144.0240]], grad_fn=<AddmmBackward0>) tensor([82.])\n",
      "epoch: 44 loss: 3846.97119140625\n",
      "tensor([[-13.6902]], grad_fn=<AddmmBackward0>) tensor([90.])\n",
      "epoch: 45 loss: 10751.6484375\n",
      "tensor([[22.8624]], grad_fn=<AddmmBackward0>) tensor([205.])\n",
      "epoch: 46 loss: 33174.125\n",
      "tensor([[5.5395]], grad_fn=<AddmmBackward0>) tensor([18.])\n",
      "epoch: 47 loss: 155.26490783691406\n",
      "tensor([[102.6682]], grad_fn=<AddmmBackward0>) tensor([56.])\n",
      "epoch: 48 loss: 2177.919189453125\n",
      "tensor([[48.3867]], grad_fn=<AddmmBackward0>) tensor([143.])\n",
      "epoch: 49 loss: 8951.669921875\n",
      "tensor([[93.6055]], grad_fn=<AddmmBackward0>) tensor([48.])\n",
      "epoch: 50 loss: 2079.8623046875\n",
      "tensor([[118.8481]], grad_fn=<AddmmBackward0>) tensor([178.])\n",
      "epoch: 51 loss: 3498.9482421875\n",
      "tensor([[159.9150]], grad_fn=<AddmmBackward0>) tensor([65.])\n",
      "epoch: 52 loss: 9008.8583984375\n",
      "tensor([[110.3910]], grad_fn=<AddmmBackward0>) tensor([115.])\n",
      "epoch: 53 loss: 21.242820739746094\n",
      "tensor([[64.0025]], grad_fn=<AddmmBackward0>) tensor([95.])\n",
      "epoch: 54 loss: 960.8429565429688\n",
      "tensor([[54.6934]], grad_fn=<AddmmBackward0>) tensor([55.])\n",
      "epoch: 55 loss: 0.09401677548885345\n",
      "tensor([[441.6843]], grad_fn=<AddmmBackward0>) tensor([194.])\n",
      "epoch: 56 loss: 61347.52734375\n",
      "tensor([[-2.8850]], grad_fn=<AddmmBackward0>) tensor([177.])\n",
      "epoch: 57 loss: 32358.6171875\n",
      "tensor([[-24.8883]], grad_fn=<AddmmBackward0>) tensor([134.])\n",
      "epoch: 58 loss: 25245.474609375\n",
      "tensor([[-11.6079]], grad_fn=<AddmmBackward0>) tensor([129.])\n",
      "epoch: 59 loss: 19770.583984375\n",
      "tensor([[-6.9998]], grad_fn=<AddmmBackward0>) tensor([157.])\n",
      "epoch: 60 loss: 26895.935546875\n",
      "tensor([[3.4271]], grad_fn=<AddmmBackward0>) tensor([58.])\n",
      "epoch: 61 loss: 2978.206787109375\n",
      "tensor([[14.4679]], grad_fn=<AddmmBackward0>) tensor([17.])\n",
      "epoch: 62 loss: 6.41129732131958\n",
      "tensor([[34.5577]], grad_fn=<AddmmBackward0>) tensor([191.])\n",
      "epoch: 63 loss: 24474.19140625\n",
      "tensor([[47.7201]], grad_fn=<AddmmBackward0>) tensor([69.])\n",
      "epoch: 64 loss: 452.8360900878906\n",
      "tensor([[81.3410]], grad_fn=<AddmmBackward0>) tensor([141.])\n",
      "epoch: 65 loss: 3559.200439453125\n",
      "tensor([[118.1762]], grad_fn=<AddmmBackward0>) tensor([133.])\n",
      "epoch: 66 loss: 219.74411010742188\n",
      "tensor([[209.2071]], grad_fn=<AddmmBackward0>) tensor([204.])\n",
      "epoch: 67 loss: 27.113492965698242\n",
      "tensor([[185.9403]], grad_fn=<AddmmBackward0>) tensor([111.])\n",
      "epoch: 68 loss: 5616.04296875\n",
      "tensor([[213.0165]], grad_fn=<AddmmBackward0>) tensor([189.])\n",
      "epoch: 69 loss: 576.792724609375\n",
      "tensor([[203.2469]], grad_fn=<AddmmBackward0>) tensor([176.])\n",
      "epoch: 70 loss: 742.3928833007812\n",
      "tensor([[168.5142]], grad_fn=<AddmmBackward0>) tensor([33.])\n",
      "epoch: 71 loss: 18364.107421875\n",
      "tensor([[137.2903]], grad_fn=<AddmmBackward0>) tensor([74.])\n",
      "epoch: 72 loss: 4005.663818359375\n",
      "tensor([[100.8863]], grad_fn=<AddmmBackward0>) tensor([146.])\n",
      "epoch: 73 loss: 2035.2467041015625\n",
      "tensor([[75.6360]], grad_fn=<AddmmBackward0>) tensor([190.])\n",
      "epoch: 74 loss: 13079.12109375\n",
      "tensor([[64.6008]], grad_fn=<AddmmBackward0>) tensor([22.])\n",
      "epoch: 75 loss: 1814.826171875\n",
      "tensor([[57.2912]], grad_fn=<AddmmBackward0>) tensor([21.])\n",
      "epoch: 76 loss: 1317.052490234375\n",
      "tensor([[45.8664]], grad_fn=<AddmmBackward0>) tensor([7.])\n",
      "epoch: 77 loss: 1510.5975341796875\n",
      "tensor([[33.1548]], grad_fn=<AddmmBackward0>) tensor([120.])\n",
      "epoch: 78 loss: 7542.0888671875\n",
      "tensor([[32.4708]], grad_fn=<AddmmBackward0>) tensor([148.])\n",
      "epoch: 79 loss: 13346.9951171875\n",
      "tensor([[43.8062]], grad_fn=<AddmmBackward0>) tensor([20.])\n",
      "epoch: 80 loss: 566.7334594726562\n",
      "tensor([[56.7179]], grad_fn=<AddmmBackward0>) tensor([88.])\n",
      "epoch: 81 loss: 978.5722045898438\n",
      "tensor([[66.8173]], grad_fn=<AddmmBackward0>) tensor([12.])\n",
      "epoch: 82 loss: 3004.941162109375\n",
      "tensor([[71.8873]], grad_fn=<AddmmBackward0>) tensor([44.])\n",
      "epoch: 83 loss: 777.7018432617188\n",
      "tensor([[81.2284]], grad_fn=<AddmmBackward0>) tensor([153.])\n",
      "epoch: 84 loss: 5151.16552734375\n",
      "tensor([[89.1348]], grad_fn=<AddmmBackward0>) tensor([102.])\n",
      "epoch: 85 loss: 165.51327514648438\n",
      "tensor([[103.5294]], grad_fn=<AddmmBackward0>) tensor([149.])\n",
      "epoch: 86 loss: 2067.57861328125\n",
      "tensor([[119.1833]], grad_fn=<AddmmBackward0>) tensor([113.])\n",
      "epoch: 87 loss: 38.23362350463867\n",
      "tensor([[133.7763]], grad_fn=<AddmmBackward0>) tensor([110.])\n",
      "epoch: 88 loss: 565.312744140625\n",
      "tensor([[146.6296]], grad_fn=<AddmmBackward0>) tensor([142.])\n",
      "epoch: 89 loss: 21.433271408081055\n",
      "tensor([[164.0673]], grad_fn=<AddmmBackward0>) tensor([186.])\n",
      "epoch: 90 loss: 481.04302978515625\n",
      "tensor([[150.4178]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "epoch: 91 loss: 22325.693359375\n",
      "tensor([[153.4529]], grad_fn=<AddmmBackward0>) tensor([144.])\n",
      "epoch: 92 loss: 89.35753631591797\n",
      "tensor([[131.6313]], grad_fn=<AddmmBackward0>) tensor([84.])\n",
      "epoch: 93 loss: 2268.739501953125\n",
      "tensor([[114.6623]], grad_fn=<AddmmBackward0>) tensor([78.])\n",
      "epoch: 94 loss: 1344.1220703125\n",
      "tensor([[110.5056]], grad_fn=<AddmmBackward0>) tensor([180.])\n",
      "epoch: 95 loss: 4829.47265625\n",
      "tensor([[86.1404]], grad_fn=<AddmmBackward0>) tensor([10.])\n",
      "epoch: 96 loss: 5797.35302734375\n",
      "tensor([[82.0232]], grad_fn=<AddmmBackward0>) tensor([126.])\n",
      "epoch: 97 loss: 1933.9608154296875\n",
      "tensor([[66.5045]], grad_fn=<AddmmBackward0>) tensor([27.])\n",
      "epoch: 98 loss: 1560.603271484375\n",
      "tensor([[57.6611]], grad_fn=<AddmmBackward0>) tensor([9.])\n",
      "epoch: 99 loss: 2367.904296875\n",
      "tensor([[56.8909]], grad_fn=<AddmmBackward0>) tensor([179.])\n",
      "epoch: 100 loss: 14910.626953125\n",
      "tensor([[58.2210]], grad_fn=<AddmmBackward0>) tensor([182.])\n",
      "epoch: 101 loss: 15321.25\n",
      "tensor([[59.9874]], grad_fn=<AddmmBackward0>) tensor([87.])\n",
      "epoch: 102 loss: 729.6826171875\n",
      "tensor([[69.1152]], grad_fn=<AddmmBackward0>) tensor([30.])\n",
      "epoch: 103 loss: 1530.0015869140625\n",
      "tensor([[84.9620]], grad_fn=<AddmmBackward0>) tensor([106.])\n",
      "epoch: 104 loss: 442.59783935546875\n",
      "tensor([[88.0004]], grad_fn=<AddmmBackward0>) tensor([52.])\n",
      "epoch: 105 loss: 1296.028076171875\n",
      "tensor([[95.3702]], grad_fn=<AddmmBackward0>) tensor([85.])\n",
      "epoch: 106 loss: 107.54012298583984\n",
      "tensor([[95.4038]], grad_fn=<AddmmBackward0>) tensor([46.])\n",
      "epoch: 107 loss: 2440.736328125\n",
      "tensor([[99.9862]], grad_fn=<AddmmBackward0>) tensor([92.])\n",
      "epoch: 108 loss: 63.77985382080078\n",
      "tensor([[91.5509]], grad_fn=<AddmmBackward0>) tensor([16.])\n",
      "epoch: 109 loss: 5707.94140625\n",
      "tensor([[86.0873]], grad_fn=<AddmmBackward0>) tensor([60.])\n",
      "epoch: 110 loss: 680.54736328125\n",
      "tensor([[79.8756]], grad_fn=<AddmmBackward0>) tensor([91.])\n",
      "epoch: 111 loss: 123.75323486328125\n",
      "tensor([[77.2712]], grad_fn=<AddmmBackward0>) tensor([132.])\n",
      "epoch: 112 loss: 2995.242919921875\n",
      "tensor([[73.8660]], grad_fn=<AddmmBackward0>) tensor([116.])\n",
      "epoch: 113 loss: 1775.2755126953125\n",
      "tensor([[77.1007]], grad_fn=<AddmmBackward0>) tensor([140.])\n",
      "epoch: 114 loss: 3956.320068359375\n",
      "tensor([[87.2672]], grad_fn=<AddmmBackward0>) tensor([162.])\n",
      "epoch: 115 loss: 5584.9931640625\n",
      "tensor([[83.4302]], grad_fn=<AddmmBackward0>) tensor([37.])\n",
      "epoch: 116 loss: 2155.76123046875\n",
      "tensor([[102.0781]], grad_fn=<AddmmBackward0>) tensor([117.])\n",
      "epoch: 117 loss: 222.66258239746094\n",
      "tensor([[98.2865]], grad_fn=<AddmmBackward0>) tensor([14.])\n",
      "epoch: 118 loss: 7104.2060546875\n",
      "tensor([[106.7582]], grad_fn=<AddmmBackward0>) tensor([98.])\n",
      "epoch: 119 loss: 76.70529174804688\n",
      "tensor([[105.9494]], grad_fn=<AddmmBackward0>) tensor([99.])\n",
      "epoch: 120 loss: 48.29450607299805\n",
      "tensor([[96.2946]], grad_fn=<AddmmBackward0>) tensor([43.])\n",
      "epoch: 121 loss: 2840.310546875\n",
      "tensor([[93.3827]], grad_fn=<AddmmBackward0>) tensor([61.])\n",
      "epoch: 122 loss: 1048.6361083984375\n",
      "tensor([[84.5985]], grad_fn=<AddmmBackward0>) tensor([50.])\n",
      "epoch: 123 loss: 1197.055419921875\n",
      "tensor([[86.7761]], grad_fn=<AddmmBackward0>) tensor([137.])\n",
      "epoch: 124 loss: 2522.441650390625\n",
      "tensor([[86.9201]], grad_fn=<AddmmBackward0>) tensor([165.])\n",
      "epoch: 125 loss: 6096.47119140625\n",
      "tensor([[89.5879]], grad_fn=<AddmmBackward0>) tensor([167.])\n",
      "epoch: 126 loss: 5992.63330078125\n",
      "tensor([[79.1613]], grad_fn=<AddmmBackward0>) tensor([34.])\n",
      "epoch: 127 loss: 2039.54443359375\n",
      "tensor([[108.4169]], grad_fn=<AddmmBackward0>) tensor([192.])\n",
      "epoch: 128 loss: 6986.130859375\n",
      "tensor([[93.7769]], grad_fn=<AddmmBackward0>) tensor([8.])\n",
      "epoch: 129 loss: 7357.6689453125\n",
      "tensor([[102.1859]], grad_fn=<AddmmBackward0>) tensor([86.])\n",
      "epoch: 130 loss: 261.9835205078125\n",
      "tensor([[103.1272]], grad_fn=<AddmmBackward0>) tensor([79.])\n",
      "epoch: 131 loss: 582.1223754882812\n",
      "tensor([[101.6573]], grad_fn=<AddmmBackward0>) tensor([67.])\n",
      "epoch: 132 loss: 1201.12548828125\n",
      "tensor([[111.7913]], grad_fn=<AddmmBackward0>) tensor([138.])\n",
      "epoch: 133 loss: 686.89404296875\n",
      "tensor([[96.0832]], grad_fn=<AddmmBackward0>) tensor([51.])\n",
      "epoch: 134 loss: 2032.493408203125\n",
      "tensor([[103.3799]], grad_fn=<AddmmBackward0>) tensor([112.])\n",
      "epoch: 135 loss: 74.30602264404297\n",
      "tensor([[101.3878]], grad_fn=<AddmmBackward0>) tensor([119.])\n",
      "epoch: 136 loss: 310.18896484375\n",
      "tensor([[89.6990]], grad_fn=<AddmmBackward0>) tensor([71.])\n",
      "epoch: 137 loss: 349.6507873535156\n",
      "tensor([[125.1898]], grad_fn=<AddmmBackward0>) tensor([198.])\n",
      "epoch: 138 loss: 5301.3212890625\n",
      "tensor([[89.7720]], grad_fn=<AddmmBackward0>) tensor([57.])\n",
      "epoch: 139 loss: 1074.004150390625\n",
      "tensor([[90.6939]], grad_fn=<AddmmBackward0>) tensor([53.])\n",
      "epoch: 140 loss: 1420.827880859375\n",
      "tensor([[99.8382]], grad_fn=<AddmmBackward0>) tensor([103.])\n",
      "epoch: 141 loss: 9.99681282043457\n",
      "tensor([[85.1177]], grad_fn=<AddmmBackward0>) tensor([0.])\n",
      "epoch: 142 loss: 7245.0146484375\n",
      "tensor([[112.5470]], grad_fn=<AddmmBackward0>) tensor([184.])\n",
      "epoch: 143 loss: 5105.5283203125\n",
      "tensor([[82.0033]], grad_fn=<AddmmBackward0>) tensor([70.])\n",
      "epoch: 144 loss: 144.0792999267578\n",
      "tensor([[90.5323]], grad_fn=<AddmmBackward0>) tensor([108.])\n",
      "epoch: 145 loss: 305.11883544921875\n",
      "tensor([[94.8602]], grad_fn=<AddmmBackward0>) tensor([136.])\n",
      "epoch: 146 loss: 1692.485107421875\n",
      "tensor([[79.4430]], grad_fn=<AddmmBackward0>) tensor([6.])\n",
      "epoch: 147 loss: 5393.86767578125\n",
      "tensor([[76.5531]], grad_fn=<AddmmBackward0>) tensor([38.])\n",
      "epoch: 148 loss: 1486.341552734375\n",
      "tensor([[102.6967]], grad_fn=<AddmmBackward0>) tensor([175.])\n",
      "epoch: 149 loss: 5227.76611328125\n",
      "tensor([[83.7845]], grad_fn=<AddmmBackward0>) tensor([93.])\n",
      "epoch: 150 loss: 84.92572021484375\n",
      "tensor([[104.9042]], grad_fn=<AddmmBackward0>) tensor([164.])\n",
      "epoch: 151 loss: 3492.309326171875\n",
      "tensor([[114.5998]], grad_fn=<AddmmBackward0>) tensor([173.])\n",
      "epoch: 152 loss: 3410.584228515625\n",
      "tensor([[101.3262]], grad_fn=<AddmmBackward0>) tensor([76.])\n",
      "epoch: 153 loss: 641.41650390625\n",
      "tensor([[105.1794]], grad_fn=<AddmmBackward0>) tensor([45.])\n",
      "epoch: 154 loss: 3621.555419921875\n",
      "tensor([[106.0392]], grad_fn=<AddmmBackward0>) tensor([15.])\n",
      "epoch: 155 loss: 8288.1318359375\n",
      "tensor([[151.4414]], grad_fn=<AddmmBackward0>) tensor([183.])\n",
      "epoch: 156 loss: 995.9429321289062\n",
      "tensor([[120.5940]], grad_fn=<AddmmBackward0>) tensor([114.])\n",
      "epoch: 157 loss: 43.4804573059082\n",
      "tensor([[133.4122]], grad_fn=<AddmmBackward0>) tensor([169.])\n",
      "epoch: 158 loss: 1266.491455078125\n",
      "tensor([[96.0860]], grad_fn=<AddmmBackward0>) tensor([24.])\n",
      "epoch: 159 loss: 5196.39453125\n",
      "tensor([[98.8672]], grad_fn=<AddmmBackward0>) tensor([75.])\n",
      "epoch: 160 loss: 569.6426391601562\n",
      "tensor([[91.7685]], grad_fn=<AddmmBackward0>) tensor([68.])\n",
      "epoch: 161 loss: 564.939453125\n",
      "tensor([[75.5336]], grad_fn=<AddmmBackward0>) tensor([39.])\n",
      "epoch: 162 loss: 1334.7044677734375\n",
      "tensor([[68.0161]], grad_fn=<AddmmBackward0>) tensor([35.])\n",
      "epoch: 163 loss: 1090.062744140625\n",
      "tensor([[121.6249]], grad_fn=<AddmmBackward0>) tensor([196.])\n",
      "epoch: 164 loss: 5531.65087890625\n",
      "tensor([[64.0780]], grad_fn=<AddmmBackward0>) tensor([81.])\n",
      "epoch: 165 loss: 286.35552978515625\n",
      "tensor([[76.0269]], grad_fn=<AddmmBackward0>) tensor([125.])\n",
      "epoch: 166 loss: 2398.3681640625\n",
      "tensor([[71.1607]], grad_fn=<AddmmBackward0>) tensor([97.])\n",
      "epoch: 167 loss: 667.6695556640625\n",
      "tensor([[78.6084]], grad_fn=<AddmmBackward0>) tensor([109.])\n",
      "epoch: 168 loss: 923.6522216796875\n",
      "tensor([[65.5860]], grad_fn=<AddmmBackward0>) tensor([23.])\n",
      "epoch: 169 loss: 1813.5692138671875\n",
      "tensor([[90.7791]], grad_fn=<AddmmBackward0>) tensor([147.])\n",
      "epoch: 170 loss: 3160.789794921875\n",
      "tensor([[108.1683]], grad_fn=<AddmmBackward0>) tensor([171.])\n",
      "epoch: 171 loss: 3947.824951171875\n",
      "tensor([[107.8561]], grad_fn=<AddmmBackward0>) tensor([145.])\n",
      "epoch: 172 loss: 1379.6719970703125\n",
      "tensor([[140.2895]], grad_fn=<AddmmBackward0>) tensor([174.])\n",
      "epoch: 173 loss: 1136.3995361328125\n",
      "tensor([[139.1577]], grad_fn=<AddmmBackward0>) tensor([118.])\n",
      "epoch: 174 loss: 447.64697265625\n",
      "tensor([[160.0096]], grad_fn=<AddmmBackward0>) tensor([124.])\n",
      "epoch: 175 loss: 1296.693359375\n",
      "tensor([[141.4384]], grad_fn=<AddmmBackward0>) tensor([83.])\n",
      "epoch: 176 loss: 3415.052001953125\n",
      "tensor([[170.1562]], grad_fn=<AddmmBackward0>) tensor([135.])\n",
      "epoch: 177 loss: 1235.9554443359375\n",
      "tensor([[221.1337]], grad_fn=<AddmmBackward0>) tensor([195.])\n",
      "epoch: 178 loss: 682.9677734375\n",
      "tensor([[113.9631]], grad_fn=<AddmmBackward0>) tensor([28.])\n",
      "epoch: 179 loss: 7389.65771484375\n",
      "tensor([[126.0513]], grad_fn=<AddmmBackward0>) tensor([105.])\n",
      "epoch: 180 loss: 443.15594482421875\n",
      "tensor([[96.8743]], grad_fn=<AddmmBackward0>) tensor([64.])\n",
      "epoch: 181 loss: 1080.721923828125\n",
      "tensor([[79.8906]], grad_fn=<AddmmBackward0>) tensor([49.])\n",
      "epoch: 182 loss: 954.230224609375\n",
      "tensor([[174.9849]], grad_fn=<AddmmBackward0>) tensor([201.])\n",
      "epoch: 183 loss: 676.787353515625\n",
      "tensor([[174.3362]], grad_fn=<AddmmBackward0>) tensor([203.])\n",
      "epoch: 184 loss: 821.614501953125\n",
      "tensor([[78.6898]], grad_fn=<AddmmBackward0>) tensor([156.])\n",
      "epoch: 185 loss: 5976.87353515625\n",
      "tensor([[49.4269]], grad_fn=<AddmmBackward0>) tensor([3.])\n",
      "epoch: 186 loss: 2155.458984375\n",
      "For epoch 2 the test accuracy over the whole test set is 0 %\n",
      "tensor([[53.1786]], grad_fn=<AddmmBackward0>) tensor([78.])\n",
      "epoch: 0 loss: 616.1007690429688\n",
      "tensor([[67.5082]], grad_fn=<AddmmBackward0>) tensor([126.])\n",
      "epoch: 1 loss: 3421.286865234375\n",
      "tensor([[71.9986]], grad_fn=<AddmmBackward0>) tensor([152.])\n",
      "epoch: 2 loss: 6400.2294921875\n",
      "tensor([[68.9632]], grad_fn=<AddmmBackward0>) tensor([101.])\n",
      "epoch: 3 loss: 1026.3572998046875\n",
      "tensor([[58.0264]], grad_fn=<AddmmBackward0>) tensor([35.])\n",
      "epoch: 4 loss: 530.2151489257812\n",
      "tensor([[101.9481]], grad_fn=<AddmmBackward0>) tensor([159.])\n",
      "epoch: 5 loss: 3254.9248046875\n",
      "tensor([[100.1384]], grad_fn=<AddmmBackward0>) tensor([122.])\n",
      "epoch: 6 loss: 477.9303283691406\n",
      "tensor([[88.1993]], grad_fn=<AddmmBackward0>) tensor([57.])\n",
      "epoch: 7 loss: 973.39599609375\n",
      "tensor([[172.3750]], grad_fn=<AddmmBackward0>) tensor([185.])\n",
      "epoch: 8 loss: 159.3914031982422\n",
      "tensor([[130.5400]], grad_fn=<AddmmBackward0>) tensor([129.])\n",
      "epoch: 9 loss: 2.371532440185547\n",
      "tensor([[139.9104]], grad_fn=<AddmmBackward0>) tensor([140.])\n",
      "epoch: 10 loss: 0.008025355637073517\n",
      "tensor([[148.0034]], grad_fn=<AddmmBackward0>) tensor([141.])\n",
      "epoch: 11 loss: 49.04743576049805\n",
      "tensor([[154.8144]], grad_fn=<AddmmBackward0>) tensor([146.])\n",
      "epoch: 12 loss: 77.69404602050781\n",
      "tensor([[199.5340]], grad_fn=<AddmmBackward0>) tensor([179.])\n",
      "epoch: 13 loss: 421.6443786621094\n",
      "tensor([[152.8223]], grad_fn=<AddmmBackward0>) tensor([108.])\n",
      "epoch: 14 loss: 2009.041015625\n",
      "tensor([[156.1336]], grad_fn=<AddmmBackward0>) tensor([135.])\n",
      "epoch: 15 loss: 446.62994384765625\n",
      "tensor([[140.8997]], grad_fn=<AddmmBackward0>) tensor([103.])\n",
      "epoch: 16 loss: 1436.389892578125\n",
      "tensor([[115.5341]], grad_fn=<AddmmBackward0>) tensor([82.])\n",
      "epoch: 17 loss: 1124.5345458984375\n",
      "tensor([[128.2223]], grad_fn=<AddmmBackward0>) tensor([136.])\n",
      "epoch: 18 loss: 60.493011474609375\n",
      "tensor([[131.2262]], grad_fn=<AddmmBackward0>) tensor([157.])\n",
      "epoch: 19 loss: 664.2874145507812\n",
      "tensor([[125.9615]], grad_fn=<AddmmBackward0>) tensor([167.])\n",
      "epoch: 20 loss: 1684.1571044921875\n",
      "tensor([[79.2318]], grad_fn=<AddmmBackward0>) tensor([17.])\n",
      "epoch: 21 loss: 3872.79736328125\n",
      "tensor([[74.5866]], grad_fn=<AddmmBackward0>) tensor([11.])\n",
      "epoch: 22 loss: 4043.2568359375\n",
      "tensor([[147.4969]], grad_fn=<AddmmBackward0>) tensor([186.])\n",
      "epoch: 23 loss: 1482.488525390625\n",
      "tensor([[103.8614]], grad_fn=<AddmmBackward0>) tensor([162.])\n",
      "epoch: 24 loss: 3380.1025390625\n",
      "tensor([[61.5466]], grad_fn=<AddmmBackward0>) tensor([15.])\n",
      "epoch: 25 loss: 2166.5849609375\n",
      "tensor([[58.7650]], grad_fn=<AddmmBackward0>) tensor([32.])\n",
      "epoch: 26 loss: 716.3639526367188\n",
      "tensor([[102.6165]], grad_fn=<AddmmBackward0>) tensor([166.])\n",
      "epoch: 27 loss: 4017.468017578125\n",
      "tensor([[88.6119]], grad_fn=<AddmmBackward0>) tensor([132.])\n",
      "epoch: 28 loss: 1882.530517578125\n",
      "tensor([[85.0819]], grad_fn=<AddmmBackward0>) tensor([95.])\n",
      "epoch: 29 loss: 98.36852264404297\n",
      "tensor([[150.1044]], grad_fn=<AddmmBackward0>) tensor([176.])\n",
      "epoch: 30 loss: 670.5797119140625\n",
      "tensor([[182.6058]], grad_fn=<AddmmBackward0>) tensor([180.])\n",
      "epoch: 31 loss: 6.79021692276001\n",
      "tensor([[168.6809]], grad_fn=<AddmmBackward0>) tensor([173.])\n",
      "epoch: 32 loss: 18.654817581176758\n",
      "tensor([[149.0000]], grad_fn=<AddmmBackward0>) tensor([113.])\n",
      "epoch: 33 loss: 1295.9989013671875\n",
      "tensor([[108.7197]], grad_fn=<AddmmBackward0>) tensor([37.])\n",
      "epoch: 34 loss: 5143.71044921875\n",
      "tensor([[157.7948]], grad_fn=<AddmmBackward0>) tensor([130.])\n",
      "epoch: 35 loss: 772.5526123046875\n",
      "tensor([[113.0612]], grad_fn=<AddmmBackward0>) tensor([85.])\n",
      "epoch: 36 loss: 787.4302368164062\n",
      "tensor([[93.6185]], grad_fn=<AddmmBackward0>) tensor([53.])\n",
      "epoch: 37 loss: 1649.8643798828125\n",
      "tensor([[84.0130]], grad_fn=<AddmmBackward0>) tensor([81.])\n",
      "epoch: 38 loss: 9.078310012817383\n",
      "tensor([[61.1883]], grad_fn=<AddmmBackward0>) tensor([46.])\n",
      "epoch: 39 loss: 230.68402099609375\n",
      "tensor([[56.8710]], grad_fn=<AddmmBackward0>) tensor([84.])\n",
      "epoch: 40 loss: 735.9852294921875\n",
      "tensor([[62.8398]], grad_fn=<AddmmBackward0>) tensor([97.])\n",
      "epoch: 41 loss: 1166.919189453125\n",
      "tensor([[52.5502]], grad_fn=<AddmmBackward0>) tensor([89.])\n",
      "epoch: 42 loss: 1328.586669921875\n",
      "tensor([[72.3936]], grad_fn=<AddmmBackward0>) tensor([124.])\n",
      "epoch: 43 loss: 2663.218017578125\n",
      "tensor([[47.6762]], grad_fn=<AddmmBackward0>) tensor([76.])\n",
      "epoch: 44 loss: 802.2386474609375\n",
      "tensor([[109.9381]], grad_fn=<AddmmBackward0>) tensor([168.])\n",
      "epoch: 45 loss: 3371.18115234375\n",
      "tensor([[85.4273]], grad_fn=<AddmmBackward0>) tensor([96.])\n",
      "epoch: 46 loss: 111.78215789794922\n",
      "tensor([[195.3382]], grad_fn=<AddmmBackward0>) tensor([193.])\n",
      "epoch: 47 loss: 5.466945648193359\n",
      "tensor([[134.1106]], grad_fn=<AddmmBackward0>) tensor([151.])\n",
      "epoch: 48 loss: 285.2530212402344\n",
      "tensor([[138.4766]], grad_fn=<AddmmBackward0>) tensor([117.])\n",
      "epoch: 49 loss: 461.2460021972656\n",
      "tensor([[158.8927]], grad_fn=<AddmmBackward0>) tensor([145.])\n",
      "epoch: 50 loss: 193.0062713623047\n",
      "tensor([[154.0394]], grad_fn=<AddmmBackward0>) tensor([110.])\n",
      "epoch: 51 loss: 1939.47265625\n",
      "tensor([[105.1046]], grad_fn=<AddmmBackward0>) tensor([25.])\n",
      "epoch: 52 loss: 6416.74072265625\n",
      "tensor([[318.7758]], grad_fn=<AddmmBackward0>) tensor([201.])\n",
      "epoch: 53 loss: 13871.150390625\n",
      "tensor([[118.7609]], grad_fn=<AddmmBackward0>) tensor([142.])\n",
      "epoch: 54 loss: 540.0552978515625\n",
      "tensor([[94.7290]], grad_fn=<AddmmBackward0>) tensor([144.])\n",
      "epoch: 55 loss: 2427.627197265625\n",
      "tensor([[46.4759]], grad_fn=<AddmmBackward0>) tensor([52.])\n",
      "epoch: 56 loss: 30.515947341918945\n",
      "tensor([[81.4652]], grad_fn=<AddmmBackward0>) tensor([170.])\n",
      "epoch: 57 loss: 7838.4130859375\n",
      "tensor([[28.5862]], grad_fn=<AddmmBackward0>) tensor([34.])\n",
      "epoch: 58 loss: 29.309553146362305\n",
      "tensor([[111.2502]], grad_fn=<AddmmBackward0>) tensor([197.])\n",
      "epoch: 59 loss: 7353.03515625\n",
      "tensor([[57.2132]], grad_fn=<AddmmBackward0>) tensor([116.])\n",
      "epoch: 60 loss: 3455.88916015625\n",
      "tensor([[45.8701]], grad_fn=<AddmmBackward0>) tensor([79.])\n",
      "epoch: 61 loss: 1097.587646484375\n",
      "tensor([[48.3043]], grad_fn=<AddmmBackward0>) tensor([50.])\n",
      "epoch: 62 loss: 2.8753910064697266\n",
      "tensor([[48.9470]], grad_fn=<AddmmBackward0>) tensor([38.])\n",
      "epoch: 63 loss: 119.83594512939453\n",
      "tensor([[54.6243]], grad_fn=<AddmmBackward0>) tensor([24.])\n",
      "epoch: 64 loss: 937.8490600585938\n",
      "tensor([[69.2658]], grad_fn=<AddmmBackward0>) tensor([80.])\n",
      "epoch: 65 loss: 115.22386169433594\n",
      "tensor([[62.7414]], grad_fn=<AddmmBackward0>) tensor([0.])\n",
      "epoch: 66 loss: 3936.4853515625\n",
      "tensor([[92.9112]], grad_fn=<AddmmBackward0>) tensor([111.])\n",
      "epoch: 67 loss: 327.2040710449219\n",
      "tensor([[102.5161]], grad_fn=<AddmmBackward0>) tensor([148.])\n",
      "epoch: 68 loss: 2068.7861328125\n",
      "tensor([[73.9655]], grad_fn=<AddmmBackward0>) tensor([55.])\n",
      "epoch: 69 loss: 359.6893310546875\n",
      "tensor([[89.5968]], grad_fn=<AddmmBackward0>) tensor([88.])\n",
      "epoch: 70 loss: 2.549921989440918\n",
      "tensor([[71.1738]], grad_fn=<AddmmBackward0>) tensor([23.])\n",
      "epoch: 71 loss: 2320.71484375\n",
      "tensor([[91.1862]], grad_fn=<AddmmBackward0>) tensor([90.])\n",
      "epoch: 72 loss: 1.4070594310760498\n",
      "tensor([[142.3914]], grad_fn=<AddmmBackward0>) tensor([182.])\n",
      "epoch: 73 loss: 1568.8446044921875\n",
      "tensor([[115.1278]], grad_fn=<AddmmBackward0>) tensor([149.])\n",
      "epoch: 74 loss: 1147.3228759765625\n",
      "tensor([[111.8705]], grad_fn=<AddmmBackward0>) tensor([109.])\n",
      "epoch: 75 loss: 8.239543914794922\n",
      "tensor([[81.7979]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "epoch: 76 loss: 6528.2939453125\n",
      "tensor([[78.7454]], grad_fn=<AddmmBackward0>) tensor([20.])\n",
      "epoch: 77 loss: 3451.025634765625\n",
      "tensor([[84.3170]], grad_fn=<AddmmBackward0>) tensor([64.])\n",
      "epoch: 78 loss: 412.7821044921875\n",
      "tensor([[71.1317]], grad_fn=<AddmmBackward0>) tensor([44.])\n",
      "epoch: 79 loss: 736.1265869140625\n",
      "tensor([[100.9899]], grad_fn=<AddmmBackward0>) tensor([137.])\n",
      "epoch: 80 loss: 1296.724609375\n",
      "tensor([[55.1624]], grad_fn=<AddmmBackward0>) tensor([7.])\n",
      "epoch: 81 loss: 2319.61376953125\n",
      "tensor([[47.8629]], grad_fn=<AddmmBackward0>) tensor([9.])\n",
      "epoch: 82 loss: 1510.3240966796875\n",
      "tensor([[72.4916]], grad_fn=<AddmmBackward0>) tensor([106.])\n",
      "epoch: 83 loss: 1122.81494140625\n",
      "tensor([[131.8304]], grad_fn=<AddmmBackward0>) tensor([200.])\n",
      "epoch: 84 loss: 4647.0947265625\n",
      "tensor([[40.2773]], grad_fn=<AddmmBackward0>) tensor([48.])\n",
      "epoch: 85 loss: 59.6395378112793\n",
      "tensor([[46.1918]], grad_fn=<AddmmBackward0>) tensor([60.])\n",
      "epoch: 86 loss: 190.6668243408203\n",
      "tensor([[99.9115]], grad_fn=<AddmmBackward0>) tensor([171.])\n",
      "epoch: 87 loss: 5053.58154296875\n",
      "tensor([[42.5303]], grad_fn=<AddmmBackward0>) tensor([39.])\n",
      "epoch: 88 loss: 12.462803840637207\n",
      "tensor([[74.4363]], grad_fn=<AddmmBackward0>) tensor([93.])\n",
      "epoch: 89 loss: 344.6114501953125\n",
      "tensor([[205.8689]], grad_fn=<AddmmBackward0>) tensor([205.])\n",
      "epoch: 90 loss: 0.7549811005592346\n",
      "tensor([[61.4436]], grad_fn=<AddmmBackward0>) tensor([27.])\n",
      "epoch: 91 loss: 1186.361083984375\n",
      "tensor([[77.1615]], grad_fn=<AddmmBackward0>) tensor([58.])\n",
      "epoch: 92 loss: 367.16217041015625\n",
      "tensor([[81.0631]], grad_fn=<AddmmBackward0>) tensor([65.])\n",
      "epoch: 93 loss: 258.02276611328125\n",
      "tensor([[83.2157]], grad_fn=<AddmmBackward0>) tensor([71.])\n",
      "epoch: 94 loss: 149.2230987548828\n",
      "tensor([[64.5100]], grad_fn=<AddmmBackward0>) tensor([8.])\n",
      "epoch: 95 loss: 3193.382080078125\n",
      "tensor([[108.1576]], grad_fn=<AddmmBackward0>) tensor([120.])\n",
      "epoch: 96 loss: 140.242431640625\n",
      "tensor([[102.3571]], grad_fn=<AddmmBackward0>) tensor([115.])\n",
      "epoch: 97 loss: 159.84307861328125\n",
      "tensor([[50.5165]], grad_fn=<AddmmBackward0>) tensor([26.])\n",
      "epoch: 98 loss: 601.0570068359375\n",
      "tensor([[222.1531]], grad_fn=<AddmmBackward0>) tensor([204.])\n",
      "epoch: 99 loss: 329.536376953125\n",
      "tensor([[134.4167]], grad_fn=<AddmmBackward0>) tensor([177.])\n",
      "epoch: 100 loss: 1813.3372802734375\n",
      "tensor([[105.6898]], grad_fn=<AddmmBackward0>) tensor([154.])\n",
      "epoch: 101 loss: 2333.8720703125\n",
      "tensor([[42.2357]], grad_fn=<AddmmBackward0>) tensor([6.])\n",
      "epoch: 102 loss: 1313.023681640625\n",
      "tensor([[60.5956]], grad_fn=<AddmmBackward0>) tensor([70.])\n",
      "epoch: 103 loss: 88.44281005859375\n",
      "tensor([[215.6039]], grad_fn=<AddmmBackward0>) tensor([203.])\n",
      "epoch: 104 loss: 158.85821533203125\n",
      "tensor([[131.2555]], grad_fn=<AddmmBackward0>) tensor([172.])\n",
      "epoch: 105 loss: 1660.111083984375\n",
      "tensor([[55.2542]], grad_fn=<AddmmBackward0>) tensor([43.])\n",
      "epoch: 106 loss: 150.16598510742188\n",
      "tensor([[125.2918]], grad_fn=<AddmmBackward0>) tensor([156.])\n",
      "epoch: 107 loss: 942.994873046875\n",
      "tensor([[116.4994]], grad_fn=<AddmmBackward0>) tensor([125.])\n",
      "epoch: 108 loss: 72.25946807861328\n",
      "tensor([[58.1975]], grad_fn=<AddmmBackward0>) tensor([12.])\n",
      "epoch: 109 loss: 2134.213134765625\n",
      "tensor([[204.3623]], grad_fn=<AddmmBackward0>) tensor([189.])\n",
      "epoch: 110 loss: 236.0004119873047\n",
      "tensor([[55.8918]], grad_fn=<AddmmBackward0>) tensor([16.])\n",
      "epoch: 111 loss: 1591.353515625\n",
      "tensor([[188.0630]], grad_fn=<AddmmBackward0>) tensor([192.])\n",
      "epoch: 112 loss: 15.499580383300781\n",
      "tensor([[69.0860]], grad_fn=<AddmmBackward0>) tensor([75.])\n",
      "epoch: 113 loss: 34.97487258911133\n",
      "tensor([[65.3495]], grad_fn=<AddmmBackward0>) tensor([68.])\n",
      "epoch: 114 loss: 7.025217533111572\n",
      "tensor([[39.6295]], grad_fn=<AddmmBackward0>) tensor([29.])\n",
      "epoch: 115 loss: 112.98694610595703\n",
      "tensor([[107.7583]], grad_fn=<AddmmBackward0>) tensor([138.])\n",
      "epoch: 116 loss: 914.5631103515625\n",
      "tensor([[139.1374]], grad_fn=<AddmmBackward0>) tensor([163.])\n",
      "epoch: 117 loss: 569.4219360351562\n",
      "tensor([[184.7738]], grad_fn=<AddmmBackward0>) tensor([184.])\n",
      "epoch: 118 loss: 0.5988194346427917\n",
      "tensor([[53.3715]], grad_fn=<AddmmBackward0>) tensor([47.])\n",
      "epoch: 119 loss: 40.595550537109375\n",
      "tensor([[57.7501]], grad_fn=<AddmmBackward0>) tensor([49.])\n",
      "epoch: 120 loss: 76.56397247314453\n",
      "tensor([[43.7073]], grad_fn=<AddmmBackward0>) tensor([22.])\n",
      "epoch: 121 loss: 471.2048034667969\n",
      "tensor([[192.7602]], grad_fn=<AddmmBackward0>) tensor([190.])\n",
      "epoch: 122 loss: 7.618664741516113\n",
      "tensor([[120.3765]], grad_fn=<AddmmBackward0>) tensor([150.])\n",
      "epoch: 123 loss: 877.5497436523438\n",
      "tensor([[62.9629]], grad_fn=<AddmmBackward0>) tensor([62.])\n",
      "epoch: 124 loss: 0.9271583557128906\n",
      "tensor([[41.9942]], grad_fn=<AddmmBackward0>) tensor([5.])\n",
      "epoch: 125 loss: 1368.5706787109375\n",
      "tensor([[38.6224]], grad_fn=<AddmmBackward0>) tensor([18.])\n",
      "epoch: 126 loss: 425.2818908691406\n",
      "tensor([[53.9534]], grad_fn=<AddmmBackward0>) tensor([56.])\n",
      "epoch: 127 loss: 4.188713550567627\n",
      "tensor([[220.3624]], grad_fn=<AddmmBackward0>) tensor([198.])\n",
      "epoch: 128 loss: 500.0754089355469\n",
      "tensor([[23.4203]], grad_fn=<AddmmBackward0>) tensor([30.])\n",
      "epoch: 129 loss: 43.29268264770508\n",
      "tensor([[129.1331]], grad_fn=<AddmmBackward0>) tensor([164.])\n",
      "epoch: 130 loss: 1215.6983642578125\n",
      "tensor([[17.6312]], grad_fn=<AddmmBackward0>) tensor([14.])\n",
      "epoch: 131 loss: 13.185314178466797\n",
      "tensor([[79.4305]], grad_fn=<AddmmBackward0>) tensor([105.])\n",
      "epoch: 132 loss: 653.7994995117188\n",
      "tensor([[32.0231]], grad_fn=<AddmmBackward0>) tensor([45.])\n",
      "epoch: 133 loss: 168.4006805419922\n",
      "tensor([[67.0407]], grad_fn=<AddmmBackward0>) tensor([91.])\n",
      "epoch: 134 loss: 574.0479125976562\n",
      "tensor([[26.6226]], grad_fn=<AddmmBackward0>) tensor([13.])\n",
      "epoch: 135 loss: 185.575927734375\n",
      "tensor([[117.2245]], grad_fn=<AddmmBackward0>) tensor([139.])\n",
      "epoch: 136 loss: 474.17327880859375\n",
      "tensor([[39.1893]], grad_fn=<AddmmBackward0>) tensor([33.])\n",
      "epoch: 137 loss: 38.3072509765625\n",
      "tensor([[72.4861]], grad_fn=<AddmmBackward0>) tensor([67.])\n",
      "epoch: 138 loss: 30.097368240356445\n",
      "tensor([[48.9569]], grad_fn=<AddmmBackward0>) tensor([4.])\n",
      "epoch: 139 loss: 2021.125732421875\n",
      "tensor([[81.6661]], grad_fn=<AddmmBackward0>) tensor([86.])\n",
      "epoch: 140 loss: 18.782495498657227\n",
      "tensor([[149.3394]], grad_fn=<AddmmBackward0>) tensor([153.])\n",
      "epoch: 141 loss: 13.399871826171875\n",
      "tensor([[45.2111]], grad_fn=<AddmmBackward0>) tensor([2.])\n",
      "epoch: 142 loss: 1867.197998046875\n",
      "tensor([[39.1209]], grad_fn=<AddmmBackward0>) tensor([19.])\n",
      "epoch: 143 loss: 404.85028076171875\n",
      "tensor([[30.4109]], grad_fn=<AddmmBackward0>) tensor([21.])\n",
      "epoch: 144 loss: 88.56571960449219\n",
      "tensor([[20.9542]], grad_fn=<AddmmBackward0>) tensor([3.])\n",
      "epoch: 145 loss: 322.35443115234375\n",
      "tensor([[140.9620]], grad_fn=<AddmmBackward0>) tensor([160.])\n",
      "epoch: 146 loss: 362.4440612792969\n",
      "tensor([[94.3960]], grad_fn=<AddmmBackward0>) tensor([134.])\n",
      "epoch: 147 loss: 1568.4759521484375\n",
      "tensor([[72.6948]], grad_fn=<AddmmBackward0>) tensor([100.])\n",
      "epoch: 148 loss: 745.573486328125\n",
      "tensor([[34.2105]], grad_fn=<AddmmBackward0>) tensor([54.])\n",
      "epoch: 149 loss: 391.62298583984375\n",
      "tensor([[98.1548]], grad_fn=<AddmmBackward0>) tensor([107.])\n",
      "epoch: 150 loss: 78.23674774169922\n",
      "tensor([[201.8434]], grad_fn=<AddmmBackward0>) tensor([183.])\n",
      "epoch: 151 loss: 355.0725402832031\n",
      "tensor([[195.1433]], grad_fn=<AddmmBackward0>) tensor([195.])\n",
      "epoch: 152 loss: 0.02054665982723236\n",
      "tensor([[64.6612]], grad_fn=<AddmmBackward0>) tensor([61.])\n",
      "epoch: 153 loss: 13.4041166305542\n",
      "tensor([[223.9146]], grad_fn=<AddmmBackward0>) tensor([196.])\n",
      "epoch: 154 loss: 779.22216796875\n",
      "tensor([[211.0687]], grad_fn=<AddmmBackward0>) tensor([191.])\n",
      "epoch: 155 loss: 402.7525329589844\n",
      "tensor([[106.9582]], grad_fn=<AddmmBackward0>) tensor([104.])\n",
      "epoch: 156 loss: 8.750848770141602\n",
      "tensor([[78.2838]], grad_fn=<AddmmBackward0>) tensor([94.])\n",
      "epoch: 157 loss: 247.0004425048828\n",
      "tensor([[153.4252]], grad_fn=<AddmmBackward0>) tensor([174.])\n",
      "epoch: 158 loss: 423.3235778808594\n",
      "tensor([[165.5058]], grad_fn=<AddmmBackward0>) tensor([175.])\n",
      "epoch: 159 loss: 90.14015197753906\n",
      "tensor([[106.7135]], grad_fn=<AddmmBackward0>) tensor([119.])\n",
      "epoch: 160 loss: 150.95693969726562\n",
      "tensor([[80.8024]], grad_fn=<AddmmBackward0>) tensor([92.])\n",
      "epoch: 161 loss: 125.38627624511719\n",
      "tensor([[130.4363]], grad_fn=<AddmmBackward0>) tensor([143.])\n",
      "epoch: 162 loss: 157.8474578857422\n",
      "tensor([[35.6192]], grad_fn=<AddmmBackward0>) tensor([10.])\n",
      "epoch: 163 loss: 656.3421020507812\n",
      "tensor([[74.4256]], grad_fn=<AddmmBackward0>) tensor([83.])\n",
      "epoch: 164 loss: 73.52088928222656\n",
      "tensor([[120.6798]], grad_fn=<AddmmBackward0>) tensor([102.])\n",
      "epoch: 165 loss: 348.93414306640625\n",
      "tensor([[158.6969]], grad_fn=<AddmmBackward0>) tensor([155.])\n",
      "epoch: 166 loss: 13.667290687561035\n",
      "tensor([[144.3994]], grad_fn=<AddmmBackward0>) tensor([147.])\n",
      "epoch: 167 loss: 6.763285160064697\n",
      "tensor([[137.3795]], grad_fn=<AddmmBackward0>) tensor([133.])\n",
      "epoch: 168 loss: 19.179630279541016\n",
      "tensor([[39.6459]], grad_fn=<AddmmBackward0>) tensor([36.])\n",
      "epoch: 169 loss: 13.292898178100586\n",
      "tensor([[124.9066]], grad_fn=<AddmmBackward0>) tensor([112.])\n",
      "epoch: 170 loss: 166.57955932617188\n",
      "tensor([[173.1677]], grad_fn=<AddmmBackward0>) tensor([165.])\n",
      "epoch: 171 loss: 66.71097564697266\n",
      "tensor([[102.3166]], grad_fn=<AddmmBackward0>) tensor([99.])\n",
      "epoch: 172 loss: 10.999866485595703\n",
      "tensor([[112.3985]], grad_fn=<AddmmBackward0>) tensor([118.])\n",
      "epoch: 173 loss: 31.3770751953125\n",
      "tensor([[49.5820]], grad_fn=<AddmmBackward0>) tensor([51.])\n",
      "epoch: 174 loss: 2.010624647140503\n",
      "tensor([[57.7377]], grad_fn=<AddmmBackward0>) tensor([87.])\n",
      "epoch: 175 loss: 856.2827758789062\n",
      "tensor([[155.3595]], grad_fn=<AddmmBackward0>) tensor([194.])\n",
      "epoch: 176 loss: 1493.089599609375\n",
      "tensor([[125.7788]], grad_fn=<AddmmBackward0>) tensor([123.])\n",
      "epoch: 177 loss: 7.721565246582031\n",
      "tensor([[188.5005]], grad_fn=<AddmmBackward0>) tensor([199.])\n",
      "epoch: 178 loss: 110.23910522460938\n",
      "tensor([[119.8818]], grad_fn=<AddmmBackward0>) tensor([98.])\n",
      "epoch: 179 loss: 478.8114013671875\n",
      "tensor([[49.8876]], grad_fn=<AddmmBackward0>) tensor([28.])\n",
      "epoch: 180 loss: 479.0682067871094\n",
      "tensor([[88.5131]], grad_fn=<AddmmBackward0>) tensor([77.])\n",
      "epoch: 181 loss: 132.5514678955078\n",
      "tensor([[85.4365]], grad_fn=<AddmmBackward0>) tensor([69.])\n",
      "epoch: 182 loss: 270.15728759765625\n",
      "tensor([[187.8641]], grad_fn=<AddmmBackward0>) tensor([169.])\n",
      "epoch: 183 loss: 355.8556213378906\n",
      "tensor([[127.3451]], grad_fn=<AddmmBackward0>) tensor([114.])\n",
      "epoch: 184 loss: 178.0915069580078\n",
      "tensor([[190.4217]], grad_fn=<AddmmBackward0>) tensor([178.])\n",
      "epoch: 185 loss: 154.2980499267578\n",
      "tensor([[53.6750]], grad_fn=<AddmmBackward0>) tensor([74.])\n",
      "epoch: 186 loss: 413.1044006347656\n",
      "For epoch 3 the test accuracy over the whole test set is 0 %\n",
      "tensor([[71.5196]], grad_fn=<AddmmBackward0>) tensor([94.])\n",
      "epoch: 0 loss: 505.3680419921875\n",
      "tensor([[112.2135]], grad_fn=<AddmmBackward0>) tensor([145.])\n",
      "epoch: 1 loss: 1074.9539794921875\n",
      "tensor([[99.7997]], grad_fn=<AddmmBackward0>) tensor([110.])\n",
      "epoch: 2 loss: 104.0461654663086\n",
      "tensor([[43.0883]], grad_fn=<AddmmBackward0>) tensor([49.])\n",
      "epoch: 3 loss: 34.948299407958984\n",
      "tensor([[110.5458]], grad_fn=<AddmmBackward0>) tensor([112.])\n",
      "epoch: 4 loss: 2.114588975906372\n",
      "tensor([[26.1235]], grad_fn=<AddmmBackward0>) tensor([16.])\n",
      "epoch: 5 loss: 102.48577117919922\n",
      "tensor([[150.0540]], grad_fn=<AddmmBackward0>) tensor([155.])\n",
      "epoch: 6 loss: 24.463359832763672\n",
      "tensor([[36.4971]], grad_fn=<AddmmBackward0>) tensor([24.])\n",
      "epoch: 7 loss: 156.1763916015625\n",
      "tensor([[122.0315]], grad_fn=<AddmmBackward0>) tensor([105.])\n",
      "epoch: 8 loss: 290.0718078613281\n",
      "tensor([[35.7724]], grad_fn=<AddmmBackward0>) tensor([30.])\n",
      "epoch: 9 loss: 33.32038116455078\n",
      "tensor([[56.6013]], grad_fn=<AddmmBackward0>) tensor([43.])\n",
      "epoch: 10 loss: 184.9948272705078\n",
      "tensor([[130.4819]], grad_fn=<AddmmBackward0>) tensor([138.])\n",
      "epoch: 11 loss: 56.52109146118164\n",
      "tensor([[112.3102]], grad_fn=<AddmmBackward0>) tensor([108.])\n",
      "epoch: 12 loss: 18.577526092529297\n",
      "tensor([[123.1631]], grad_fn=<AddmmBackward0>) tensor([129.])\n",
      "epoch: 13 loss: 34.068851470947266\n",
      "tensor([[44.7190]], grad_fn=<AddmmBackward0>) tensor([44.])\n",
      "epoch: 14 loss: 0.5169416069984436\n",
      "tensor([[17.5547]], grad_fn=<AddmmBackward0>) tensor([6.])\n",
      "epoch: 15 loss: 133.5106201171875\n",
      "tensor([[155.8201]], grad_fn=<AddmmBackward0>) tensor([165.])\n",
      "epoch: 16 loss: 84.27030181884766\n",
      "tensor([[95.7746]], grad_fn=<AddmmBackward0>) tensor([102.])\n",
      "epoch: 17 loss: 38.755165100097656\n",
      "tensor([[128.4344]], grad_fn=<AddmmBackward0>) tensor([153.])\n",
      "epoch: 18 loss: 603.4678344726562\n",
      "tensor([[57.2720]], grad_fn=<AddmmBackward0>) tensor([76.])\n",
      "epoch: 19 loss: 350.7373046875\n",
      "tensor([[65.3037]], grad_fn=<AddmmBackward0>) tensor([79.])\n",
      "epoch: 20 loss: 187.58895874023438\n",
      "tensor([[127.6005]], grad_fn=<AddmmBackward0>) tensor([117.])\n",
      "epoch: 21 loss: 112.37113189697266\n",
      "tensor([[35.8498]], grad_fn=<AddmmBackward0>) tensor([15.])\n",
      "epoch: 22 loss: 434.716064453125\n",
      "tensor([[220.2397]], grad_fn=<AddmmBackward0>) tensor([182.])\n",
      "epoch: 23 loss: 1462.2781982421875\n",
      "tensor([[113.2913]], grad_fn=<AddmmBackward0>) tensor([98.])\n",
      "epoch: 24 loss: 233.82308959960938\n",
      "tensor([[133.5085]], grad_fn=<AddmmBackward0>) tensor([137.])\n",
      "epoch: 25 loss: 12.19057846069336\n",
      "tensor([[21.5575]], grad_fn=<AddmmBackward0>) tensor([35.])\n",
      "epoch: 26 loss: 180.6996612548828\n",
      "tensor([[18.6031]], grad_fn=<AddmmBackward0>) tensor([37.])\n",
      "epoch: 27 loss: 338.4465026855469\n",
      "tensor([[120.7858]], grad_fn=<AddmmBackward0>) tensor([139.])\n",
      "epoch: 28 loss: 331.7585754394531\n",
      "tensor([[111.4293]], grad_fn=<AddmmBackward0>) tensor([115.])\n",
      "epoch: 29 loss: 12.7496919631958\n",
      "tensor([[24.1563]], grad_fn=<AddmmBackward0>) tensor([39.])\n",
      "epoch: 30 loss: 220.33583068847656\n",
      "tensor([[193.9230]], grad_fn=<AddmmBackward0>) tensor([201.])\n",
      "epoch: 31 loss: 50.08430099487305\n",
      "tensor([[65.0047]], grad_fn=<AddmmBackward0>) tensor([53.])\n",
      "epoch: 32 loss: 144.11244201660156\n",
      "tensor([[213.3381]], grad_fn=<AddmmBackward0>) tensor([196.])\n",
      "epoch: 33 loss: 300.6082763671875\n",
      "tensor([[144.4698]], grad_fn=<AddmmBackward0>) tensor([143.])\n",
      "epoch: 34 loss: 2.1601858139038086\n",
      "tensor([[77.5169]], grad_fn=<AddmmBackward0>) tensor([78.])\n",
      "epoch: 35 loss: 0.233342245221138\n",
      "tensor([[38.2908]], grad_fn=<AddmmBackward0>) tensor([2.])\n",
      "epoch: 36 loss: 1317.019287109375\n",
      "tensor([[92.7574]], grad_fn=<AddmmBackward0>) tensor([93.])\n",
      "epoch: 37 loss: 0.05883230268955231\n",
      "tensor([[144.6320]], grad_fn=<AddmmBackward0>) tensor([156.])\n",
      "epoch: 38 loss: 129.2306365966797\n",
      "tensor([[179.3390]], grad_fn=<AddmmBackward0>) tensor([178.])\n",
      "epoch: 39 loss: 1.7929739952087402\n",
      "tensor([[183.3392]], grad_fn=<AddmmBackward0>) tensor([186.])\n",
      "epoch: 40 loss: 7.08000373840332\n",
      "tensor([[123.2486]], grad_fn=<AddmmBackward0>) tensor([146.])\n",
      "epoch: 41 loss: 517.6246337890625\n",
      "tensor([[109.3637]], grad_fn=<AddmmBackward0>) tensor([113.])\n",
      "epoch: 42 loss: 13.222393989562988\n",
      "tensor([[175.9592]], grad_fn=<AddmmBackward0>) tensor([176.])\n",
      "epoch: 43 loss: 0.001663558417931199\n",
      "tensor([[166.4252]], grad_fn=<AddmmBackward0>) tensor([172.])\n",
      "epoch: 44 loss: 31.078378677368164\n",
      "tensor([[68.9731]], grad_fn=<AddmmBackward0>) tensor([70.])\n",
      "epoch: 45 loss: 1.054510474205017\n",
      "tensor([[138.6118]], grad_fn=<AddmmBackward0>) tensor([147.])\n",
      "epoch: 46 loss: 70.36136627197266\n",
      "tensor([[135.6373]], grad_fn=<AddmmBackward0>) tensor([125.])\n",
      "epoch: 47 loss: 113.1524429321289\n",
      "tensor([[130.0950]], grad_fn=<AddmmBackward0>) tensor([114.])\n",
      "epoch: 48 loss: 259.050537109375\n",
      "tensor([[112.1443]], grad_fn=<AddmmBackward0>) tensor([97.])\n",
      "epoch: 49 loss: 229.34967041015625\n",
      "tensor([[37.2774]], grad_fn=<AddmmBackward0>) tensor([28.])\n",
      "epoch: 50 loss: 86.0709457397461\n",
      "tensor([[33.0681]], grad_fn=<AddmmBackward0>) tensor([33.])\n",
      "epoch: 51 loss: 0.004642803687602282\n",
      "tensor([[29.1024]], grad_fn=<AddmmBackward0>) tensor([10.])\n",
      "epoch: 52 loss: 364.90167236328125\n",
      "tensor([[51.8110]], grad_fn=<AddmmBackward0>) tensor([58.])\n",
      "epoch: 53 loss: 38.3040885925293\n",
      "tensor([[16.0610]], grad_fn=<AddmmBackward0>) tensor([4.])\n",
      "epoch: 54 loss: 145.46783447265625\n",
      "tensor([[82.2540]], grad_fn=<AddmmBackward0>) tensor([100.])\n",
      "epoch: 55 loss: 314.9203186035156\n",
      "tensor([[9.2984]], grad_fn=<AddmmBackward0>) tensor([26.])\n",
      "epoch: 56 loss: 278.9443054199219\n",
      "tensor([[8.3841]], grad_fn=<AddmmBackward0>) tensor([7.])\n",
      "epoch: 57 loss: 1.9157617092132568\n",
      "tensor([[48.5518]], grad_fn=<AddmmBackward0>) tensor([81.])\n",
      "epoch: 58 loss: 1052.885009765625\n",
      "tensor([[88.4436]], grad_fn=<AddmmBackward0>) tensor([99.])\n",
      "epoch: 59 loss: 111.43814849853516\n",
      "tensor([[190.2847]], grad_fn=<AddmmBackward0>) tensor([200.])\n",
      "epoch: 60 loss: 94.38738250732422\n",
      "tensor([[195.2900]], grad_fn=<AddmmBackward0>) tensor([184.])\n",
      "epoch: 61 loss: 127.46498107910156\n",
      "tensor([[76.1785]], grad_fn=<AddmmBackward0>) tensor([85.])\n",
      "epoch: 62 loss: 77.81917572021484\n",
      "tensor([[203.4096]], grad_fn=<AddmmBackward0>) tensor([191.])\n",
      "epoch: 63 loss: 153.9979705810547\n",
      "tensor([[105.3866]], grad_fn=<AddmmBackward0>) tensor([89.])\n",
      "epoch: 64 loss: 268.52081298828125\n",
      "tensor([[161.2402]], grad_fn=<AddmmBackward0>) tensor([154.])\n",
      "epoch: 65 loss: 52.42033004760742\n",
      "tensor([[41.7996]], grad_fn=<AddmmBackward0>) tensor([9.])\n",
      "epoch: 66 loss: 1075.815673828125\n",
      "tensor([[176.6804]], grad_fn=<AddmmBackward0>) tensor([174.])\n",
      "epoch: 67 loss: 7.184732913970947\n",
      "tensor([[72.5434]], grad_fn=<AddmmBackward0>) tensor([87.])\n",
      "epoch: 68 loss: 208.99339294433594\n",
      "tensor([[54.2716]], grad_fn=<AddmmBackward0>) tensor([46.])\n",
      "epoch: 69 loss: 68.4189682006836\n",
      "tensor([[178.4309]], grad_fn=<AddmmBackward0>) tensor([203.])\n",
      "epoch: 70 loss: 603.6409912109375\n",
      "tensor([[65.6522]], grad_fn=<AddmmBackward0>) tensor([82.])\n",
      "epoch: 71 loss: 267.2491149902344\n",
      "tensor([[165.6558]], grad_fn=<AddmmBackward0>) tensor([164.])\n",
      "epoch: 72 loss: 2.7416985034942627\n",
      "tensor([[34.6728]], grad_fn=<AddmmBackward0>) tensor([22.])\n",
      "epoch: 73 loss: 160.59922790527344\n",
      "tensor([[133.9613]], grad_fn=<AddmmBackward0>) tensor([135.])\n",
      "epoch: 74 loss: 1.0788899660110474\n",
      "tensor([[134.8721]], grad_fn=<AddmmBackward0>) tensor([136.])\n",
      "epoch: 75 loss: 1.2722598314285278\n",
      "tensor([[172.2114]], grad_fn=<AddmmBackward0>) tensor([162.])\n",
      "epoch: 76 loss: 104.27290344238281\n",
      "tensor([[68.5079]], grad_fn=<AddmmBackward0>) tensor([64.])\n",
      "epoch: 77 loss: 20.321199417114258\n",
      "tensor([[139.3847]], grad_fn=<AddmmBackward0>) tensor([152.])\n",
      "epoch: 78 loss: 159.14605712890625\n",
      "tensor([[129.5434]], grad_fn=<AddmmBackward0>) tensor([123.])\n",
      "epoch: 79 loss: 42.816429138183594\n",
      "tensor([[170.1605]], grad_fn=<AddmmBackward0>) tensor([169.])\n",
      "epoch: 80 loss: 1.3466707468032837\n",
      "tensor([[136.5846]], grad_fn=<AddmmBackward0>) tensor([144.])\n",
      "epoch: 81 loss: 54.98755645751953\n",
      "tensor([[109.7920]], grad_fn=<AddmmBackward0>) tensor([101.])\n",
      "epoch: 82 loss: 77.2996597290039\n",
      "tensor([[69.6315]], grad_fn=<AddmmBackward0>) tensor([71.])\n",
      "epoch: 83 loss: 1.8726872205734253\n",
      "tensor([[28.5670]], grad_fn=<AddmmBackward0>) tensor([17.])\n",
      "epoch: 84 loss: 133.79437255859375\n",
      "tensor([[68.0856]], grad_fn=<AddmmBackward0>) tensor([83.])\n",
      "epoch: 85 loss: 222.44064331054688\n",
      "tensor([[57.5155]], grad_fn=<AddmmBackward0>) tensor([54.])\n",
      "epoch: 86 loss: 12.35908317565918\n",
      "tensor([[115.5761]], grad_fn=<AddmmBackward0>) tensor([109.])\n",
      "epoch: 87 loss: 43.244632720947266\n",
      "tensor([[115.8642]], grad_fn=<AddmmBackward0>) tensor([111.])\n",
      "epoch: 88 loss: 23.65996551513672\n",
      "tensor([[86.6835]], grad_fn=<AddmmBackward0>) tensor([91.])\n",
      "epoch: 89 loss: 18.632219314575195\n",
      "tensor([[22.8933]], grad_fn=<AddmmBackward0>) tensor([5.])\n",
      "epoch: 90 loss: 320.17083740234375\n",
      "tensor([[93.7469]], grad_fn=<AddmmBackward0>) tensor([96.])\n",
      "epoch: 91 loss: 5.076414108276367\n",
      "tensor([[122.8624]], grad_fn=<AddmmBackward0>) tensor([149.])\n",
      "epoch: 92 loss: 683.1727294921875\n",
      "tensor([[21.3653]], grad_fn=<AddmmBackward0>) tensor([25.])\n",
      "epoch: 93 loss: 13.210980415344238\n",
      "tensor([[19.4109]], grad_fn=<AddmmBackward0>) tensor([14.])\n",
      "epoch: 94 loss: 29.2783145904541\n",
      "tensor([[129.8162]], grad_fn=<AddmmBackward0>) tensor([150.])\n",
      "epoch: 95 loss: 407.3854675292969\n",
      "tensor([[117.8580]], grad_fn=<AddmmBackward0>) tensor([116.])\n",
      "epoch: 96 loss: 3.4521703720092773\n",
      "tensor([[61.2679]], grad_fn=<AddmmBackward0>) tensor([51.])\n",
      "epoch: 97 loss: 105.43052673339844\n",
      "tensor([[206.1936]], grad_fn=<AddmmBackward0>) tensor([192.])\n",
      "epoch: 98 loss: 201.4588165283203\n",
      "tensor([[72.2445]], grad_fn=<AddmmBackward0>) tensor([77.])\n",
      "epoch: 99 loss: 22.615005493164062\n",
      "tensor([[70.2919]], grad_fn=<AddmmBackward0>) tensor([75.])\n",
      "epoch: 100 loss: 22.166486740112305\n",
      "tensor([[94.3519]], grad_fn=<AddmmBackward0>) tensor([88.])\n",
      "epoch: 101 loss: 40.346900939941406\n",
      "tensor([[138.3993]], grad_fn=<AddmmBackward0>) tensor([140.])\n",
      "epoch: 102 loss: 2.562363862991333\n",
      "tensor([[119.2618]], grad_fn=<AddmmBackward0>) tensor([107.])\n",
      "epoch: 103 loss: 150.3521728515625\n",
      "tensor([[64.8805]], grad_fn=<AddmmBackward0>) tensor([67.])\n",
      "epoch: 104 loss: 4.492115020751953\n",
      "tensor([[102.2547]], grad_fn=<AddmmBackward0>) tensor([95.])\n",
      "epoch: 105 loss: 52.6301155090332\n",
      "tensor([[109.1283]], grad_fn=<AddmmBackward0>) tensor([103.])\n",
      "epoch: 106 loss: 37.55563735961914\n",
      "tensor([[56.4019]], grad_fn=<AddmmBackward0>) tensor([65.])\n",
      "epoch: 107 loss: 73.92743682861328\n",
      "tensor([[227.0919]], grad_fn=<AddmmBackward0>) tensor([199.])\n",
      "epoch: 108 loss: 789.1576538085938\n",
      "tensor([[149.2366]], grad_fn=<AddmmBackward0>) tensor([157.])\n",
      "epoch: 109 loss: 60.26962661743164\n",
      "tensor([[106.7502]], grad_fn=<AddmmBackward0>) tensor([130.])\n",
      "epoch: 110 loss: 540.5543212890625\n",
      "tensor([[144.2796]], grad_fn=<AddmmBackward0>) tensor([173.])\n",
      "epoch: 111 loss: 824.8638916015625\n",
      "tensor([[174.0152]], grad_fn=<AddmmBackward0>) tensor([185.])\n",
      "epoch: 112 loss: 120.66520690917969\n",
      "tensor([[16.7339]], grad_fn=<AddmmBackward0>) tensor([19.])\n",
      "epoch: 113 loss: 5.135010242462158\n",
      "tensor([[186.9302]], grad_fn=<AddmmBackward0>) tensor([183.])\n",
      "epoch: 114 loss: 15.44664192199707\n",
      "tensor([[221.3784]], grad_fn=<AddmmBackward0>) tensor([204.])\n",
      "epoch: 115 loss: 302.0093994140625\n",
      "tensor([[28.8081]], grad_fn=<AddmmBackward0>) tensor([36.])\n",
      "epoch: 116 loss: 51.72389602661133\n",
      "tensor([[30.2382]], grad_fn=<AddmmBackward0>) tensor([27.])\n",
      "epoch: 117 loss: 10.485847473144531\n",
      "tensor([[175.2321]], grad_fn=<AddmmBackward0>) tensor([195.])\n",
      "epoch: 118 loss: 390.7710266113281\n",
      "tensor([[133.5626]], grad_fn=<AddmmBackward0>) tensor([124.])\n",
      "epoch: 119 loss: 91.44286346435547\n",
      "tensor([[207.1398]], grad_fn=<AddmmBackward0>) tensor([198.])\n",
      "epoch: 120 loss: 83.53512573242188\n",
      "tensor([[196.7177]], grad_fn=<AddmmBackward0>) tensor([179.])\n",
      "epoch: 121 loss: 313.9173278808594\n",
      "tensor([[68.6867]], grad_fn=<AddmmBackward0>) tensor([62.])\n",
      "epoch: 122 loss: 44.71234893798828\n",
      "tensor([[129.5527]], grad_fn=<AddmmBackward0>) tensor([126.])\n",
      "epoch: 123 loss: 12.621487617492676\n",
      "tensor([[29.2643]], grad_fn=<AddmmBackward0>) tensor([34.])\n",
      "epoch: 124 loss: 22.426408767700195\n",
      "tensor([[84.4586]], grad_fn=<AddmmBackward0>) tensor([92.])\n",
      "epoch: 125 loss: 56.873130798339844\n",
      "tensor([[24.6956]], grad_fn=<AddmmBackward0>) tensor([12.])\n",
      "epoch: 126 loss: 161.17933654785156\n",
      "tensor([[104.5295]], grad_fn=<AddmmBackward0>) tensor([106.])\n",
      "epoch: 127 loss: 2.162294387817383\n",
      "tensor([[120.5700]], grad_fn=<AddmmBackward0>) tensor([151.])\n",
      "epoch: 128 loss: 925.9844360351562\n",
      "tensor([[162.2407]], grad_fn=<AddmmBackward0>) tensor([189.])\n",
      "epoch: 129 loss: 716.05810546875\n",
      "tensor([[29.8852]], grad_fn=<AddmmBackward0>) tensor([32.])\n",
      "epoch: 130 loss: 4.472376823425293\n",
      "tensor([[64.5177]], grad_fn=<AddmmBackward0>) tensor([55.])\n",
      "epoch: 131 loss: 90.586181640625\n",
      "tensor([[138.3020]], grad_fn=<AddmmBackward0>) tensor([132.])\n",
      "epoch: 132 loss: 39.71503448486328\n",
      "tensor([[131.9831]], grad_fn=<AddmmBackward0>) tensor([118.])\n",
      "epoch: 133 loss: 195.5260467529297\n",
      "tensor([[72.2447]], grad_fn=<AddmmBackward0>) tensor([52.])\n",
      "epoch: 134 loss: 409.84716796875\n",
      "tensor([[39.2099]], grad_fn=<AddmmBackward0>) tensor([13.])\n",
      "epoch: 135 loss: 686.9584350585938\n",
      "tensor([[189.0558]], grad_fn=<AddmmBackward0>) tensor([180.])\n",
      "epoch: 136 loss: 82.00836944580078\n",
      "tensor([[60.3407]], grad_fn=<AddmmBackward0>) tensor([61.])\n",
      "epoch: 137 loss: 0.43473413586616516\n",
      "tensor([[170.7555]], grad_fn=<AddmmBackward0>) tensor([197.])\n",
      "epoch: 138 loss: 688.7757568359375\n",
      "tensor([[43.3574]], grad_fn=<AddmmBackward0>) tensor([45.])\n",
      "epoch: 139 loss: 2.6982007026672363\n",
      "tensor([[160.5149]], grad_fn=<AddmmBackward0>) tensor([193.])\n",
      "epoch: 140 loss: 1055.283203125\n",
      "tensor([[126.6807]], grad_fn=<AddmmBackward0>) tensor([141.])\n",
      "epoch: 141 loss: 205.04359436035156\n",
      "tensor([[70.1924]], grad_fn=<AddmmBackward0>) tensor([86.])\n",
      "epoch: 142 loss: 249.88003540039062\n",
      "tensor([[140.5322]], grad_fn=<AddmmBackward0>) tensor([133.])\n",
      "epoch: 143 loss: 56.73420715332031\n",
      "tensor([[192.8151]], grad_fn=<AddmmBackward0>) tensor([159.])\n",
      "epoch: 144 loss: 1143.45849609375\n",
      "tensor([[42.7702]], grad_fn=<AddmmBackward0>) tensor([23.])\n",
      "epoch: 145 loss: 390.86151123046875\n",
      "tensor([[136.9835]], grad_fn=<AddmmBackward0>) tensor([120.])\n",
      "epoch: 146 loss: 288.439453125\n",
      "tensor([[65.0007]], grad_fn=<AddmmBackward0>) tensor([56.])\n",
      "epoch: 147 loss: 81.01304626464844\n",
      "tensor([[172.0983]], grad_fn=<AddmmBackward0>) tensor([166.])\n",
      "epoch: 148 loss: 37.189598083496094\n",
      "tensor([[17.6081]], grad_fn=<AddmmBackward0>) tensor([18.])\n",
      "epoch: 149 loss: 0.15355053544044495\n",
      "tensor([[154.7497]], grad_fn=<AddmmBackward0>) tensor([168.])\n",
      "epoch: 150 loss: 175.57139587402344\n",
      "tensor([[7.9732]], grad_fn=<AddmmBackward0>) tensor([38.])\n",
      "epoch: 151 loss: 901.60595703125\n",
      "tensor([[35.6588]], grad_fn=<AddmmBackward0>) tensor([48.])\n",
      "epoch: 152 loss: 152.30572509765625\n",
      "tensor([[51.6206]], grad_fn=<AddmmBackward0>) tensor([68.])\n",
      "epoch: 153 loss: 268.2838134765625\n",
      "tensor([[57.4314]], grad_fn=<AddmmBackward0>) tensor([69.])\n",
      "epoch: 154 loss: 133.83152770996094\n",
      "tensor([[134.1557]], grad_fn=<AddmmBackward0>) tensor([148.])\n",
      "epoch: 155 loss: 191.6637725830078\n",
      "tensor([[208.0910]], grad_fn=<AddmmBackward0>) tensor([194.])\n",
      "epoch: 156 loss: 198.55508422851562\n",
      "tensor([[67.7328]], grad_fn=<AddmmBackward0>) tensor([50.])\n",
      "epoch: 157 loss: 314.4533996582031\n",
      "tensor([[141.6826]], grad_fn=<AddmmBackward0>) tensor([122.])\n",
      "epoch: 158 loss: 387.406005859375\n",
      "tensor([[35.0874]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "epoch: 159 loss: 1161.95361328125\n",
      "tensor([[28.5914]], grad_fn=<AddmmBackward0>) tensor([3.])\n",
      "epoch: 160 loss: 654.9188842773438\n",
      "tensor([[22.2011]], grad_fn=<AddmmBackward0>) tensor([11.])\n",
      "epoch: 161 loss: 125.46463012695312\n",
      "tensor([[9.2595]], grad_fn=<AddmmBackward0>) tensor([8.])\n",
      "epoch: 162 loss: 1.5862863063812256\n",
      "tensor([[61.1287]], grad_fn=<AddmmBackward0>) tensor([90.])\n",
      "epoch: 163 loss: 833.5523681640625\n",
      "tensor([[147.5110]], grad_fn=<AddmmBackward0>) tensor([177.])\n",
      "epoch: 164 loss: 869.6010131835938\n",
      "tensor([[38.0227]], grad_fn=<AddmmBackward0>) tensor([80.])\n",
      "epoch: 165 loss: 1762.0955810546875\n",
      "tensor([[-0.0207]], grad_fn=<AddmmBackward0>) tensor([0.])\n",
      "epoch: 166 loss: 0.00042807214776985347\n",
      "tensor([[46.7977]], grad_fn=<AddmmBackward0>) tensor([60.])\n",
      "epoch: 167 loss: 174.30116271972656\n",
      "tensor([[162.3303]], grad_fn=<AddmmBackward0>) tensor([160.])\n",
      "epoch: 168 loss: 5.4304728507995605\n",
      "tensor([[218.8945]], grad_fn=<AddmmBackward0>) tensor([205.])\n",
      "epoch: 169 loss: 193.0575714111328\n",
      "tensor([[139.7876]], grad_fn=<AddmmBackward0>) tensor([142.])\n",
      "epoch: 170 loss: 4.894859313964844\n",
      "tensor([[33.2655]], grad_fn=<AddmmBackward0>) tensor([29.])\n",
      "epoch: 171 loss: 18.194515228271484\n",
      "tensor([[33.9409]], grad_fn=<AddmmBackward0>) tensor([21.])\n",
      "epoch: 172 loss: 167.46665954589844\n",
      "tensor([[78.2138]], grad_fn=<AddmmBackward0>) tensor([84.])\n",
      "epoch: 173 loss: 33.47959899902344\n",
      "tensor([[184.4191]], grad_fn=<AddmmBackward0>) tensor([170.])\n",
      "epoch: 174 loss: 207.9090576171875\n",
      "tensor([[33.9076]], grad_fn=<AddmmBackward0>) tensor([20.])\n",
      "epoch: 175 loss: 193.42018127441406\n",
      "tensor([[58.0711]], grad_fn=<AddmmBackward0>) tensor([47.])\n",
      "epoch: 176 loss: 122.56938934326172\n",
      "tensor([[129.6599]], grad_fn=<AddmmBackward0>) tensor([134.])\n",
      "epoch: 177 loss: 18.836759567260742\n",
      "tensor([[178.9949]], grad_fn=<AddmmBackward0>) tensor([175.])\n",
      "epoch: 178 loss: 15.95901107788086\n",
      "tensor([[60.0889]], grad_fn=<AddmmBackward0>) tensor([74.])\n",
      "epoch: 179 loss: 193.51791381835938\n",
      "tensor([[104.6913]], grad_fn=<AddmmBackward0>) tensor([104.])\n",
      "epoch: 180 loss: 0.47795820236206055\n",
      "tensor([[159.0122]], grad_fn=<AddmmBackward0>) tensor([163.])\n",
      "epoch: 181 loss: 15.902857780456543\n",
      "tensor([[182.5929]], grad_fn=<AddmmBackward0>) tensor([190.])\n",
      "epoch: 182 loss: 54.86565017700195\n",
      "tensor([[50.7432]], grad_fn=<AddmmBackward0>) tensor([57.])\n",
      "epoch: 183 loss: 39.147377014160156\n",
      "tensor([[113.4407]], grad_fn=<AddmmBackward0>) tensor([119.])\n",
      "epoch: 184 loss: 30.906023025512695\n",
      "tensor([[166.0879]], grad_fn=<AddmmBackward0>) tensor([171.])\n",
      "epoch: 185 loss: 24.1286678314209\n",
      "tensor([[169.6187]], grad_fn=<AddmmBackward0>) tensor([167.])\n",
      "epoch: 186 loss: 6.857579708099365\n",
      "For epoch 4 the test accuracy over the whole test set is 0 %\n",
      "tensor([[200.9842]], grad_fn=<AddmmBackward0>) tensor([184.])\n",
      "epoch: 0 loss: 288.46173095703125\n",
      "tensor([[68.4699]], grad_fn=<AddmmBackward0>) tensor([79.])\n",
      "epoch: 1 loss: 110.88328552246094\n",
      "tensor([[186.8973]], grad_fn=<AddmmBackward0>) tensor([177.])\n",
      "epoch: 2 loss: 97.95641326904297\n",
      "tensor([[116.0300]], grad_fn=<AddmmBackward0>) tensor([106.])\n",
      "epoch: 3 loss: 100.60163879394531\n",
      "tensor([[131.6366]], grad_fn=<AddmmBackward0>) tensor([147.])\n",
      "epoch: 4 loss: 236.0355682373047\n",
      "tensor([[65.1656]], grad_fn=<AddmmBackward0>) tensor([84.])\n",
      "epoch: 5 loss: 354.7359313964844\n",
      "tensor([[21.6350]], grad_fn=<AddmmBackward0>) tensor([9.])\n",
      "epoch: 6 loss: 159.64256286621094\n",
      "tensor([[132.0013]], grad_fn=<AddmmBackward0>) tensor([138.])\n",
      "epoch: 7 loss: 35.984619140625\n",
      "tensor([[129.0728]], grad_fn=<AddmmBackward0>) tensor([125.])\n",
      "epoch: 8 loss: 16.58794593811035\n",
      "tensor([[60.8838]], grad_fn=<AddmmBackward0>) tensor([50.])\n",
      "epoch: 9 loss: 118.45661163330078\n",
      "tensor([[137.3257]], grad_fn=<AddmmBackward0>) tensor([142.])\n",
      "epoch: 10 loss: 21.848806381225586\n",
      "tensor([[116.4329]], grad_fn=<AddmmBackward0>) tensor([105.])\n",
      "epoch: 11 loss: 130.71136474609375\n",
      "tensor([[103.9696]], grad_fn=<AddmmBackward0>) tensor([98.])\n",
      "epoch: 12 loss: 35.636634826660156\n",
      "tensor([[25.6482]], grad_fn=<AddmmBackward0>) tensor([33.])\n",
      "epoch: 13 loss: 54.04964828491211\n",
      "tensor([[19.8358]], grad_fn=<AddmmBackward0>) tensor([7.])\n",
      "epoch: 14 loss: 164.7582550048828\n",
      "tensor([[103.0062]], grad_fn=<AddmmBackward0>) tensor([102.])\n",
      "epoch: 15 loss: 1.0123517513275146\n",
      "tensor([[19.3004]], grad_fn=<AddmmBackward0>) tensor([28.])\n",
      "epoch: 16 loss: 75.6822509765625\n",
      "tensor([[45.2607]], grad_fn=<AddmmBackward0>) tensor([49.])\n",
      "epoch: 17 loss: 13.982505798339844\n",
      "tensor([[114.0658]], grad_fn=<AddmmBackward0>) tensor([123.])\n",
      "epoch: 18 loss: 79.8204116821289\n",
      "tensor([[155.1942]], grad_fn=<AddmmBackward0>) tensor([203.])\n",
      "epoch: 19 loss: 2285.39453125\n",
      "tensor([[193.0705]], grad_fn=<AddmmBackward0>) tensor([186.])\n",
      "epoch: 20 loss: 49.99169158935547\n",
      "tensor([[190.2795]], grad_fn=<AddmmBackward0>) tensor([176.])\n",
      "epoch: 21 loss: 203.9044189453125\n",
      "tensor([[142.2905]], grad_fn=<AddmmBackward0>) tensor([136.])\n",
      "epoch: 22 loss: 39.56996536254883\n",
      "tensor([[132.2347]], grad_fn=<AddmmBackward0>) tensor([115.])\n",
      "epoch: 23 loss: 297.0336608886719\n",
      "tensor([[147.3040]], grad_fn=<AddmmBackward0>) tensor([143.])\n",
      "epoch: 24 loss: 18.524423599243164\n",
      "tensor([[123.1433]], grad_fn=<AddmmBackward0>) tensor([107.])\n",
      "epoch: 25 loss: 260.6069641113281\n",
      "tensor([[153.0616]], grad_fn=<AddmmBackward0>) tensor([156.])\n",
      "epoch: 26 loss: 8.634285926818848\n",
      "tensor([[90.2875]], grad_fn=<AddmmBackward0>) tensor([93.])\n",
      "epoch: 27 loss: 7.357499122619629\n",
      "tensor([[21.6296]], grad_fn=<AddmmBackward0>) tensor([5.])\n",
      "epoch: 28 loss: 276.5447692871094\n",
      "tensor([[17.6291]], grad_fn=<AddmmBackward0>) tensor([16.])\n",
      "epoch: 29 loss: 2.653963088989258\n",
      "tensor([[76.8474]], grad_fn=<AddmmBackward0>) tensor([88.])\n",
      "epoch: 30 loss: 124.38124084472656\n",
      "tensor([[186.7704]], grad_fn=<AddmmBackward0>) tensor([185.])\n",
      "epoch: 31 loss: 3.134373664855957\n",
      "tensor([[13.7906]], grad_fn=<AddmmBackward0>) tensor([36.])\n",
      "epoch: 32 loss: 493.2577819824219\n",
      "tensor([[108.0776]], grad_fn=<AddmmBackward0>) tensor([126.])\n",
      "epoch: 33 loss: 321.2108459472656\n",
      "tensor([[150.1918]], grad_fn=<AddmmBackward0>) tensor([168.])\n",
      "epoch: 34 loss: 317.13079833984375\n",
      "tensor([[157.0564]], grad_fn=<AddmmBackward0>) tensor([160.])\n",
      "epoch: 35 loss: 8.664801597595215\n",
      "tensor([[21.9607]], grad_fn=<AddmmBackward0>) tensor([14.])\n",
      "epoch: 36 loss: 63.37257766723633\n",
      "tensor([[103.2320]], grad_fn=<AddmmBackward0>) tensor([95.])\n",
      "epoch: 37 loss: 67.76548767089844\n",
      "tensor([[132.5077]], grad_fn=<AddmmBackward0>) tensor([132.])\n",
      "epoch: 38 loss: 0.257734090089798\n",
      "tensor([[97.1180]], grad_fn=<AddmmBackward0>) tensor([91.])\n",
      "epoch: 39 loss: 37.42987823486328\n",
      "tensor([[119.8778]], grad_fn=<AddmmBackward0>) tensor([104.])\n",
      "epoch: 40 loss: 252.10525512695312\n",
      "tensor([[72.8157]], grad_fn=<AddmmBackward0>) tensor([69.])\n",
      "epoch: 41 loss: 14.55965805053711\n",
      "tensor([[119.6346]], grad_fn=<AddmmBackward0>) tensor([113.])\n",
      "epoch: 42 loss: 44.01748275756836\n",
      "tensor([[189.7016]], grad_fn=<AddmmBackward0>) tensor([178.])\n",
      "epoch: 43 loss: 136.92849731445312\n",
      "tensor([[52.9468]], grad_fn=<AddmmBackward0>) tensor([52.])\n",
      "epoch: 44 loss: 0.8965173363685608\n",
      "tensor([[105.3062]], grad_fn=<AddmmBackward0>) tensor([116.])\n",
      "epoch: 45 loss: 114.35772705078125\n",
      "tensor([[17.7612]], grad_fn=<AddmmBackward0>) tensor([3.])\n",
      "epoch: 46 loss: 217.89212036132812\n",
      "tensor([[51.3476]], grad_fn=<AddmmBackward0>) tensor([76.])\n",
      "epoch: 47 loss: 607.7401123046875\n",
      "tensor([[51.4839]], grad_fn=<AddmmBackward0>) tensor([71.])\n",
      "epoch: 48 loss: 380.8783874511719\n",
      "tensor([[19.1505]], grad_fn=<AddmmBackward0>) tensor([38.])\n",
      "epoch: 49 loss: 355.3033752441406\n",
      "tensor([[119.4152]], grad_fn=<AddmmBackward0>) tensor([130.])\n",
      "epoch: 50 loss: 112.03816986083984\n",
      "tensor([[24.5889]], grad_fn=<AddmmBackward0>) tensor([0.])\n",
      "epoch: 51 loss: 604.6162109375\n",
      "tensor([[255.7722]], grad_fn=<AddmmBackward0>) tensor([199.])\n",
      "epoch: 52 loss: 3223.079345703125\n",
      "tensor([[124.3292]], grad_fn=<AddmmBackward0>) tensor([149.])\n",
      "epoch: 53 loss: 608.6475830078125\n",
      "tensor([[118.9160]], grad_fn=<AddmmBackward0>) tensor([146.])\n",
      "epoch: 54 loss: 733.544677734375\n",
      "tensor([[105.8061]], grad_fn=<AddmmBackward0>) tensor([120.])\n",
      "epoch: 55 loss: 201.46595764160156\n",
      "tensor([[120.9779]], grad_fn=<AddmmBackward0>) tensor([141.])\n",
      "epoch: 56 loss: 400.88580322265625\n",
      "tensor([[31.9867]], grad_fn=<AddmmBackward0>) tensor([30.])\n",
      "epoch: 57 loss: 3.947007417678833\n",
      "tensor([[36.9015]], grad_fn=<AddmmBackward0>) tensor([34.])\n",
      "epoch: 58 loss: 8.41877269744873\n",
      "tensor([[108.3733]], grad_fn=<AddmmBackward0>) tensor([100.])\n",
      "epoch: 59 loss: 70.11212921142578\n",
      "tensor([[99.7654]], grad_fn=<AddmmBackward0>) tensor([94.])\n",
      "epoch: 60 loss: 33.23970413208008\n",
      "tensor([[99.6404]], grad_fn=<AddmmBackward0>) tensor([90.])\n",
      "epoch: 61 loss: 92.93797302246094\n",
      "tensor([[44.0348]], grad_fn=<AddmmBackward0>) tensor([11.])\n",
      "epoch: 62 loss: 1091.2999267578125\n",
      "tensor([[129.0603]], grad_fn=<AddmmBackward0>) tensor([134.])\n",
      "epoch: 63 loss: 24.400306701660156\n",
      "tensor([[33.6577]], grad_fn=<AddmmBackward0>) tensor([6.])\n",
      "epoch: 64 loss: 764.9481201171875\n",
      "tensor([[153.5455]], grad_fn=<AddmmBackward0>) tensor([166.])\n",
      "epoch: 65 loss: 155.11453247070312\n",
      "tensor([[174.8003]], grad_fn=<AddmmBackward0>) tensor([191.])\n",
      "epoch: 66 loss: 262.4309997558594\n",
      "tensor([[103.3983]], grad_fn=<AddmmBackward0>) tensor([109.])\n",
      "epoch: 67 loss: 31.379554748535156\n",
      "tensor([[133.8314]], grad_fn=<AddmmBackward0>) tensor([155.])\n",
      "epoch: 68 loss: 448.10809326171875\n",
      "tensor([[176.4029]], grad_fn=<AddmmBackward0>) tensor([183.])\n",
      "epoch: 69 loss: 43.521820068359375\n",
      "tensor([[32.7457]], grad_fn=<AddmmBackward0>) tensor([22.])\n",
      "epoch: 70 loss: 115.46984100341797\n",
      "tensor([[57.2255]], grad_fn=<AddmmBackward0>) tensor([47.])\n",
      "epoch: 71 loss: 104.56096649169922\n",
      "tensor([[33.1195]], grad_fn=<AddmmBackward0>) tensor([20.])\n",
      "epoch: 72 loss: 172.12156677246094\n",
      "tensor([[70.8717]], grad_fn=<AddmmBackward0>) tensor([68.])\n",
      "epoch: 73 loss: 8.246421813964844\n",
      "tensor([[144.2381]], grad_fn=<AddmmBackward0>) tensor([153.])\n",
      "epoch: 74 loss: 76.7701187133789\n",
      "tensor([[168.7255]], grad_fn=<AddmmBackward0>) tensor([162.])\n",
      "epoch: 75 loss: 45.23227310180664\n",
      "tensor([[119.4656]], grad_fn=<AddmmBackward0>) tensor([118.])\n",
      "epoch: 76 loss: 2.147891044616699\n",
      "tensor([[132.4931]], grad_fn=<AddmmBackward0>) tensor([137.])\n",
      "epoch: 77 loss: 20.312257766723633\n",
      "tensor([[54.0205]], grad_fn=<AddmmBackward0>) tensor([46.])\n",
      "epoch: 78 loss: 64.32909393310547\n",
      "tensor([[198.8884]], grad_fn=<AddmmBackward0>) tensor([189.])\n",
      "epoch: 79 loss: 97.78130340576172\n",
      "tensor([[53.1220]], grad_fn=<AddmmBackward0>) tensor([54.])\n",
      "epoch: 80 loss: 0.7708207964897156\n",
      "tensor([[56.0748]], grad_fn=<AddmmBackward0>) tensor([61.])\n",
      "epoch: 81 loss: 24.25753402709961\n",
      "tensor([[51.4165]], grad_fn=<AddmmBackward0>) tensor([58.])\n",
      "epoch: 82 loss: 43.34302520751953\n",
      "tensor([[178.1263]], grad_fn=<AddmmBackward0>) tensor([195.])\n",
      "epoch: 83 loss: 284.7213439941406\n",
      "tensor([[185.7248]], grad_fn=<AddmmBackward0>) tensor([193.])\n",
      "epoch: 84 loss: 52.92842102050781\n",
      "tensor([[65.7612]], grad_fn=<AddmmBackward0>) tensor([77.])\n",
      "epoch: 85 loss: 126.31165313720703\n",
      "tensor([[60.8909]], grad_fn=<AddmmBackward0>) tensor([55.])\n",
      "epoch: 86 loss: 34.702789306640625\n",
      "tensor([[200.0321]], grad_fn=<AddmmBackward0>) tensor([180.])\n",
      "epoch: 87 loss: 401.28582763671875\n",
      "tensor([[75.6733]], grad_fn=<AddmmBackward0>) tensor([85.])\n",
      "epoch: 88 loss: 86.98673248291016\n",
      "tensor([[110.9934]], grad_fn=<AddmmBackward0>) tensor([96.])\n",
      "epoch: 89 loss: 224.8011474609375\n",
      "tensor([[135.3630]], grad_fn=<AddmmBackward0>) tensor([122.])\n",
      "epoch: 90 loss: 178.5695343017578\n",
      "tensor([[91.1704]], grad_fn=<AddmmBackward0>) tensor([89.])\n",
      "epoch: 91 loss: 4.710448265075684\n",
      "tensor([[104.8169]], grad_fn=<AddmmBackward0>) tensor([101.])\n",
      "epoch: 92 loss: 14.569091796875\n",
      "tensor([[16.9755]], grad_fn=<AddmmBackward0>) tensor([18.])\n",
      "epoch: 93 loss: 1.0496625900268555\n",
      "tensor([[116.3004]], grad_fn=<AddmmBackward0>) tensor([124.])\n",
      "epoch: 94 loss: 59.283138275146484\n",
      "tensor([[55.8389]], grad_fn=<AddmmBackward0>) tensor([82.])\n",
      "epoch: 95 loss: 684.4034423828125\n",
      "tensor([[157.8237]], grad_fn=<AddmmBackward0>) tensor([167.])\n",
      "epoch: 96 loss: 84.20476531982422\n",
      "tensor([[189.7088]], grad_fn=<AddmmBackward0>) tensor([192.])\n",
      "epoch: 97 loss: 5.249591827392578\n",
      "tensor([[219.0290]], grad_fn=<AddmmBackward0>) tensor([204.])\n",
      "epoch: 98 loss: 225.8696746826172\n",
      "tensor([[64.6621]], grad_fn=<AddmmBackward0>) tensor([74.])\n",
      "epoch: 99 loss: 87.19719696044922\n",
      "tensor([[62.3145]], grad_fn=<AddmmBackward0>) tensor([62.])\n",
      "epoch: 100 loss: 0.09891195595264435\n",
      "tensor([[171.8670]], grad_fn=<AddmmBackward0>) tensor([172.])\n",
      "epoch: 101 loss: 0.01769188977777958\n",
      "tensor([[198.7506]], grad_fn=<AddmmBackward0>) tensor([201.])\n",
      "epoch: 102 loss: 5.059616565704346\n",
      "tensor([[64.7186]], grad_fn=<AddmmBackward0>) tensor([57.])\n",
      "epoch: 103 loss: 59.577335357666016\n",
      "tensor([[72.4155]], grad_fn=<AddmmBackward0>) tensor([75.])\n",
      "epoch: 104 loss: 6.6797356605529785\n",
      "tensor([[108.1517]], grad_fn=<AddmmBackward0>) tensor([97.])\n",
      "epoch: 105 loss: 124.36064910888672\n",
      "tensor([[69.1919]], grad_fn=<AddmmBackward0>) tensor([65.])\n",
      "epoch: 106 loss: 17.572044372558594\n",
      "tensor([[25.9269]], grad_fn=<AddmmBackward0>) tensor([1.])\n",
      "epoch: 107 loss: 621.3485717773438\n",
      "tensor([[115.5089]], grad_fn=<AddmmBackward0>) tensor([117.])\n",
      "epoch: 108 loss: 2.223345994949341\n",
      "tensor([[182.6946]], grad_fn=<AddmmBackward0>) tensor([200.])\n",
      "epoch: 109 loss: 299.4775695800781\n",
      "tensor([[109.8789]], grad_fn=<AddmmBackward0>) tensor([111.])\n",
      "epoch: 110 loss: 1.2567999362945557\n",
      "tensor([[18.3265]], grad_fn=<AddmmBackward0>) tensor([12.])\n",
      "epoch: 111 loss: 40.025108337402344\n",
      "tensor([[45.2677]], grad_fn=<AddmmBackward0>) tensor([48.])\n",
      "epoch: 112 loss: 7.465712547302246\n",
      "tensor([[54.7201]], grad_fn=<AddmmBackward0>) tensor([64.])\n",
      "epoch: 113 loss: 86.11738586425781\n",
      "tensor([[16.0009]], grad_fn=<AddmmBackward0>) tensor([19.])\n",
      "epoch: 114 loss: 8.994885444641113\n",
      "tensor([[13.6333]], grad_fn=<AddmmBackward0>) tensor([2.])\n",
      "epoch: 115 loss: 135.33470153808594\n",
      "tensor([[162.2481]], grad_fn=<AddmmBackward0>) tensor([173.])\n",
      "epoch: 116 loss: 115.60350799560547\n",
      "tensor([[202.4689]], grad_fn=<AddmmBackward0>) tensor([197.])\n",
      "epoch: 117 loss: 29.909229278564453\n",
      "tensor([[201.5507]], grad_fn=<AddmmBackward0>) tensor([198.])\n",
      "epoch: 118 loss: 12.60728931427002\n",
      "tensor([[199.2800]], grad_fn=<AddmmBackward0>) tensor([196.])\n",
      "epoch: 119 loss: 10.75819206237793\n",
      "tensor([[103.8714]], grad_fn=<AddmmBackward0>) tensor([103.])\n",
      "epoch: 120 loss: 0.759415864944458\n",
      "tensor([[138.3213]], grad_fn=<AddmmBackward0>) tensor([154.])\n",
      "epoch: 121 loss: 245.82244873046875\n",
      "tensor([[16.7900]], grad_fn=<AddmmBackward0>) tensor([37.])\n",
      "epoch: 122 loss: 408.44329833984375\n",
      "tensor([[21.1703]], grad_fn=<AddmmBackward0>) tensor([25.])\n",
      "epoch: 123 loss: 14.666444778442383\n",
      "tensor([[174.8372]], grad_fn=<AddmmBackward0>) tensor([165.])\n",
      "epoch: 124 loss: 96.77058410644531\n",
      "tensor([[28.6370]], grad_fn=<AddmmBackward0>) tensor([32.])\n",
      "epoch: 125 loss: 11.30962085723877\n",
      "tensor([[60.2130]], grad_fn=<AddmmBackward0>) tensor([43.])\n",
      "epoch: 126 loss: 296.2859802246094\n",
      "tensor([[30.3005]], grad_fn=<AddmmBackward0>) tensor([10.])\n",
      "epoch: 127 loss: 412.1092529296875\n",
      "tensor([[212.1926]], grad_fn=<AddmmBackward0>) tensor([205.])\n",
      "epoch: 128 loss: 51.734100341796875\n",
      "tensor([[137.6048]], grad_fn=<AddmmBackward0>) tensor([150.])\n",
      "epoch: 129 loss: 153.6417999267578\n",
      "tensor([[176.7259]], grad_fn=<AddmmBackward0>) tensor([175.])\n",
      "epoch: 130 loss: 2.978647470474243\n",
      "tensor([[53.0351]], grad_fn=<AddmmBackward0>) tensor([51.])\n",
      "epoch: 131 loss: 4.141550540924072\n",
      "tensor([[130.9028]], grad_fn=<AddmmBackward0>) tensor([145.])\n",
      "epoch: 132 loss: 198.73187255859375\n",
      "tensor([[62.3992]], grad_fn=<AddmmBackward0>) tensor([83.])\n",
      "epoch: 133 loss: 424.3923034667969\n",
      "tensor([[16.9960]], grad_fn=<AddmmBackward0>) tensor([4.])\n",
      "epoch: 134 loss: 168.89642333984375\n",
      "tensor([[135.5159]], grad_fn=<AddmmBackward0>) tensor([140.])\n",
      "epoch: 135 loss: 20.10729217529297\n",
      "tensor([[59.6975]], grad_fn=<AddmmBackward0>) tensor([53.])\n",
      "epoch: 136 loss: 44.85622787475586\n",
      "tensor([[65.0816]], grad_fn=<AddmmBackward0>) tensor([60.])\n",
      "epoch: 137 loss: 25.82254409790039\n",
      "tensor([[177.5451]], grad_fn=<AddmmBackward0>) tensor([171.])\n",
      "epoch: 138 loss: 42.838600158691406\n",
      "tensor([[19.8322]], grad_fn=<AddmmBackward0>) tensor([15.])\n",
      "epoch: 139 loss: 23.3500919342041\n",
      "tensor([[68.0319]], grad_fn=<AddmmBackward0>) tensor([78.])\n",
      "epoch: 140 loss: 99.36365509033203\n",
      "tensor([[19.9598]], grad_fn=<AddmmBackward0>) tensor([13.])\n",
      "epoch: 141 loss: 48.43837356567383\n",
      "tensor([[116.3540]], grad_fn=<AddmmBackward0>) tensor([108.])\n",
      "epoch: 142 loss: 69.78900146484375\n",
      "tensor([[63.4779]], grad_fn=<AddmmBackward0>) tensor([80.])\n",
      "epoch: 143 loss: 272.98126220703125\n",
      "tensor([[138.9758]], grad_fn=<AddmmBackward0>) tensor([152.])\n",
      "epoch: 144 loss: 169.63058471679688\n",
      "tensor([[20.8068]], grad_fn=<AddmmBackward0>) tensor([35.])\n",
      "epoch: 145 loss: 201.44744873046875\n",
      "tensor([[146.5181]], grad_fn=<AddmmBackward0>) tensor([151.])\n",
      "epoch: 146 loss: 20.08759117126465\n",
      "tensor([[142.5583]], grad_fn=<AddmmBackward0>) tensor([133.])\n",
      "epoch: 147 loss: 91.36087799072266\n",
      "tensor([[65.0472]], grad_fn=<AddmmBackward0>) tensor([45.])\n",
      "epoch: 148 loss: 401.8891296386719\n",
      "tensor([[212.3572]], grad_fn=<AddmmBackward0>) tensor([190.])\n",
      "epoch: 149 loss: 499.8440856933594\n",
      "tensor([[97.5331]], grad_fn=<AddmmBackward0>) tensor([92.])\n",
      "epoch: 150 loss: 30.614900588989258\n",
      "tensor([[52.5960]], grad_fn=<AddmmBackward0>) tensor([44.])\n",
      "epoch: 151 loss: 73.89168548583984\n",
      "tensor([[16.5214]], grad_fn=<AddmmBackward0>) tensor([27.])\n",
      "epoch: 152 loss: 109.80024719238281\n",
      "tensor([[126.3698]], grad_fn=<AddmmBackward0>) tensor([144.])\n",
      "epoch: 153 loss: 310.8243408203125\n",
      "tensor([[156.7227]], grad_fn=<AddmmBackward0>) tensor([169.])\n",
      "epoch: 154 loss: 150.73092651367188\n",
      "tensor([[110.4978]], grad_fn=<AddmmBackward0>) tensor([119.])\n",
      "epoch: 155 loss: 72.28749084472656\n",
      "tensor([[15.0683]], grad_fn=<AddmmBackward0>) tensor([21.])\n",
      "epoch: 156 loss: 35.18505096435547\n",
      "tensor([[64.6730]], grad_fn=<AddmmBackward0>) tensor([81.])\n",
      "epoch: 157 loss: 266.571044921875\n",
      "tensor([[174.2104]], grad_fn=<AddmmBackward0>) tensor([170.])\n",
      "epoch: 158 loss: 17.72736930847168\n",
      "tensor([[179.4816]], grad_fn=<AddmmBackward0>) tensor([164.])\n",
      "epoch: 159 loss: 239.67892456054688\n",
      "tensor([[143.2982]], grad_fn=<AddmmBackward0>) tensor([135.])\n",
      "epoch: 160 loss: 68.85940551757812\n",
      "tensor([[75.1562]], grad_fn=<AddmmBackward0>) tensor([70.])\n",
      "epoch: 161 loss: 26.586048126220703\n",
      "tensor([[126.7806]], grad_fn=<AddmmBackward0>) tensor([114.])\n",
      "epoch: 162 loss: 163.3431854248047\n",
      "tensor([[27.4238]], grad_fn=<AddmmBackward0>) tensor([39.])\n",
      "epoch: 163 loss: 134.00726318359375\n",
      "tensor([[133.3080]], grad_fn=<AddmmBackward0>) tensor([139.])\n",
      "epoch: 164 loss: 32.39939880371094\n",
      "tensor([[23.0943]], grad_fn=<AddmmBackward0>) tensor([17.])\n",
      "epoch: 165 loss: 37.140716552734375\n",
      "tensor([[101.4708]], grad_fn=<AddmmBackward0>) tensor([99.])\n",
      "epoch: 166 loss: 6.104788780212402\n",
      "tensor([[174.7856]], grad_fn=<AddmmBackward0>) tensor([179.])\n",
      "epoch: 167 loss: 17.760921478271484\n",
      "tensor([[126.0935]], grad_fn=<AddmmBackward0>) tensor([129.])\n",
      "epoch: 168 loss: 8.447619438171387\n",
      "tensor([[25.2358]], grad_fn=<AddmmBackward0>) tensor([23.])\n",
      "epoch: 169 loss: 4.998903274536133\n",
      "tensor([[161.3905]], grad_fn=<AddmmBackward0>) tensor([159.])\n",
      "epoch: 170 loss: 5.714431285858154\n",
      "tensor([[24.4655]], grad_fn=<AddmmBackward0>) tensor([29.])\n",
      "epoch: 171 loss: 20.56141471862793\n",
      "tensor([[168.5941]], grad_fn=<AddmmBackward0>) tensor([194.])\n",
      "epoch: 172 loss: 645.4620361328125\n",
      "tensor([[70.8964]], grad_fn=<AddmmBackward0>) tensor([87.])\n",
      "epoch: 173 loss: 259.326171875\n",
      "tensor([[72.4795]], grad_fn=<AddmmBackward0>) tensor([67.])\n",
      "epoch: 174 loss: 30.02483367919922\n",
      "tensor([[183.1252]], grad_fn=<AddmmBackward0>) tensor([174.])\n",
      "epoch: 175 loss: 83.26869201660156\n",
      "tensor([[184.3214]], grad_fn=<AddmmBackward0>) tensor([157.])\n",
      "epoch: 176 loss: 746.4578247070312\n",
      "tensor([[40.4117]], grad_fn=<AddmmBackward0>) tensor([26.])\n",
      "epoch: 177 loss: 207.69735717773438\n",
      "tensor([[125.2548]], grad_fn=<AddmmBackward0>) tensor([112.])\n",
      "epoch: 178 loss: 175.6890869140625\n",
      "tensor([[180.9811]], grad_fn=<AddmmBackward0>) tensor([182.])\n",
      "epoch: 179 loss: 1.038199782371521\n",
      "tensor([[112.5036]], grad_fn=<AddmmBackward0>) tensor([110.])\n",
      "epoch: 180 loss: 6.268247604370117\n",
      "tensor([[124.3785]], grad_fn=<AddmmBackward0>) tensor([148.])\n",
      "epoch: 181 loss: 557.9755249023438\n",
      "tensor([[15.9644]], grad_fn=<AddmmBackward0>) tensor([8.])\n",
      "epoch: 182 loss: 63.43232345581055\n",
      "tensor([[59.4148]], grad_fn=<AddmmBackward0>) tensor([86.])\n",
      "epoch: 183 loss: 706.7711181640625\n",
      "tensor([[152.7177]], grad_fn=<AddmmBackward0>) tensor([163.])\n",
      "epoch: 184 loss: 105.72480773925781\n",
      "tensor([[55.8148]], grad_fn=<AddmmBackward0>) tensor([56.])\n",
      "epoch: 185 loss: 0.03431307524442673\n",
      "tensor([[28.2883]], grad_fn=<AddmmBackward0>) tensor([24.])\n",
      "epoch: 186 loss: 18.389938354492188\n",
      "For epoch 5 the test accuracy over the whole test set is 0 %\n",
      "Finished Training\n",
      "Finished Training\n",
      "(tensor([[[[0.9961, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [0.9961, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [0.9961, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          ...,\n",
      "          [0.9961, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [0.9961, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [0.9961, 0.9961, 0.9961,  ..., 0.9961, 0.9961, 0.9961]]]]), tensor([[185.3318]]), tensor([187.]))\n"
     ]
    }
   ],
   "source": [
    "# Vdataiter=iter(valid_loader)\n",
    "# vimg, vlabels = Vdataiter.next()\n",
    "# print(vimg.shape)\n",
    "# print(vlabels.shape)\n",
    "# print(vlabels)\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    # Let's build our model\n",
    "    train(5)\n",
    "    print('Finished Training')\n",
    "\n",
    "    # Test which classes performed well\n",
    "    testAccuracy()\n",
    "    \n",
    "    # Let's load the model we just created and test the accuracy per label\n",
    "    # model = net()\n",
    "    # path = \"myFirstModel.pth\"\n",
    "    # model.load_state_dict(torch.load(path))\n",
    "\n",
    "    print(predict(net, device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch-cy1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ec9779f063a942372247cf944fedbbc91251caa904973c75af8af67dbd8132d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
