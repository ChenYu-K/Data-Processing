{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.定义数据集\n",
    "对于训练对象检测的引用脚本，实例分割和人员关键点检测要求能够轻松支持添加新的自定义数据。数据集应该从标准的类`torch.utils.data.Dataset` 继承而来，并实现`_len`和`_getitem_`\n",
    "\n",
    "我们要求的唯一特性是数据集的`__getitem__`应该返回： * 图像：PIL图像大小(H,W) * 目标：包含以下字段的字典\n",
    "<1> `boxes(FloatTensor[N,4])`：N边框（bounding boxes）坐标的格式[x0,x1,y0,y1]，取值范围是0到W,0到H。\n",
    "<2> `labels(Int64Tensor[N])`：每个边框的标签。\n",
    "<3> `image_id(Int64Tensor[1])`：图像识别器，它应该在数据集中的所有图像中是唯一的，并在评估期间使用。\n",
    "<4> `area(Tensor[N])`：边框的面积，在使用COCO指标进行评估时使用此项来分隔小、中和大框之间的度量标准得分。\n",
    "<5> `iscrowed(UInt8Tensor[N,H,W])`：在评估期间属性设置为iscrowed=True的实例会被忽略。\n",
    "<6> `(可选)masks(UInt8Tesor[N,H,W])`：每个对象的分段掩码。\n",
    "<7> `(可选)keypoints (FloatTensor[N, K, 3]`：对于N个对象中的每一个，它包含[x，y，visibility]格式的K个关键点，用 于定义对象。`visibility` = 0表示关键点不可见。请注意，对于数据扩充，翻转关键点的概念取决于数据表示，您应该调整 reference/detection/transforms.py 以用于新的关键点表示。\n",
    "\n",
    "如果你的模型返回上述方法，它们将使其适用于培训和评估，并将使用 pycocotools 的评估脚本。\n",
    "\n",
    "此外，如果要在训练期间使用宽高比分组（以便每个批次仅包含具有相似宽高比的图像），则建议还实现`get_height_and_width`方法， 该方法返回图像的高度和宽度。如果未提供此方法，我们将通过`__getitem__`查询数据集的所有元素，这会将图像加载到内存中，但比提供自定义方法时要慢。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.2 为数据集编写类"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # 下载所有图像文件，为其排序\n",
    "        # 确保它们对齐\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # 请注意我们还没有将mask转换为RGB,\n",
    "        # 因为每种颜色对应一个不同的实例\n",
    "        # 0是背景\n",
    "        mask = Image.open(mask_path)\n",
    "        # 将PIL图像转换为numpy数组\n",
    "        mask = np.array(mask)\n",
    "        # 实例被编码为不同的颜色\n",
    "        obj_ids = np.unique(mask)\n",
    "        # 第一个id是背景，所以删除它\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # 将颜色编码的mask分成一组\n",
    "        # 二进制格式\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # 获取每个mask的边界框坐标\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # 将所有转换为torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # 这里仅有一个类\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # 假设所有实例都不是人群\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. 定义模型\n",
    "现在我们需要定义一个可以上述数据集执行预测的模型。在本教程中，我们将使用 [Mask R-CNN](https://arxiv.org/abs/1703.06870)， 它基于 [Faster R-CNN](https://arxiv.org/abs/1506.01497)。Faster R-CNN 是一种模型，可以预测图像中潜在对象的边界框和类别得分。\n",
    "<img src=\"https://www.pytorch123.com/FourSection/image/03.png\" width=\"500\"/>\n",
    "Mask R-CNN 在 Faster R-CNN 中添加了一个额外的分支，它还预测每个实例的分割蒙版。\n",
    "<img src=\"https://www.pytorch123.com/FourSection/image/04.png\" width=\"500\"/>\n",
    "有两种常见情况可能需要修改`torchvision modelzoo`中的一个可用模型。第一个是我们想要从预先训练的模型开始，然后微调最后一层。 另一种是当我们想要用不同的模型替换模型的主干时（例如，用于更快的预测）。\n",
    "下面是对这两种情况的处理。 * 1 微调已经预训练的模型 让我们假设你想从一个在COCO上已预先训练过的模型开始，并希望为你的特定类进行微调。这是一种可行的方法："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# 在COCO上加载经过预训练的预训练模型\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# 将分类器替换为具有用户定义的 num_classes的新分类器\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# 获取分类器的输入参数的数量\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# 用新的头部替换预先训练好的头部\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 修改模型以添加不同的主干"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# 加载预先训练的模型进行分类和返回\n",
    "# 只有功能\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN需要知道骨干网中的输出通道数量。对于mobilenet_v2，它是1280，所以我们需要在这里添加它\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# 我们让RPN在每个空间位置生成5 x 3个锚点\n",
    "# 具有5种不同的大小和3种不同的宽高比。\n",
    "# 我们有一个元组[元组[int]]\n",
    "# 因为每个特征映射可能具有不同的大小和宽高比\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# 定义一下我们将用于执行感兴趣区域裁剪的特征映射，以及重新缩放后裁剪的大小。\n",
    "# 如果您的主干返回Tensor，则featmap_names应为[0]。\n",
    "# 更一般地，主干应该返回OrderedDict [Tensor]\n",
    "# 并且在featmap_names中，您可以选择要使用的功能映射。\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# 将这些pieces放在FasterRCNN模型中\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.1 PennFudan 数据集的实例分割模型\n",
    "\n",
    "在我们的例子中，我们希望从预先训练的模型中进行微调，因为我们的数据集非常小，所以我们将遵循上述第一种情况。\n",
    "\n",
    "这里我们还要计算实例分割掩膜，因此我们将使用 Mask R-CNN："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # 加载在COCO上预训练的预训练的实例分割模型\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # 获取分类器的输入特征数\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # 用新的头部替换预先训练好的头部\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # 现在获取掩膜分类器的输入特征数\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # 并用新的掩膜预测器替换掩膜预测器\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "就是这样，这将使模型准备好在您的自定义数据集上进行训练和评估。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.整合\n",
    "在`references/detection/`中，我们有许多辅助函数来简化训练和评估检测模型。在这里，我们将使用 `references/detection/engine.py`，`references/detection/utils.py`和`references/detection/transforms.py`。\n",
    "只需将它们复制到文件夹并在此处使用它们。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 为数据扩充/转换编写辅助函数："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 编写执行训练和验证的主要功能"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 在GPU上训练，若无GPU，可选择在CPU上训练\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # 我们的数据集只有两个类 - 背景和人\n",
    "    num_classes = 2\n",
    "    # 使用我们的数据集和定义的转换\n",
    "    dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "    dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "    # 在训练和测试集中拆分数据集\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "    # 定义训练和验证数据加载器\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    # 使用我们的辅助函数获取模型\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "    # 将我们的模型迁移到合适的设备\n",
    "    model.to(device)\n",
    "\n",
    "    # 构造一个优化器\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # 和学习率调度程序\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "    # 训练10个epochs\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练一个epoch，每10次迭代打印一次\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # 更新学习速率\n",
    "        lr_scheduler.step()\n",
    "        # 在测试集上评价\n",
    "        evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    print(\"That's it!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "'1.12.1'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'transforms' has no attribute 'ToTensor'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [35]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [34]\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     10\u001B[0m num_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# 使用我们的数据集和定义的转换\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m dataset \u001B[38;5;241m=\u001B[39m PennFudanDataset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPennFudanPed\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[43mget_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n\u001B[0;32m     13\u001B[0m dataset_test \u001B[38;5;241m=\u001B[39m PennFudanDataset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPennFudanPed\u001B[39m\u001B[38;5;124m'\u001B[39m, get_transform(train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# 在训练和测试集中拆分数据集\u001B[39;00m\n",
      "Input \u001B[1;32mIn [33]\u001B[0m, in \u001B[0;36mget_transform\u001B[1;34m(train)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_transform\u001B[39m(train):\n\u001B[0;32m      4\u001B[0m     transforms \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m----> 5\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mappend(\u001B[43mT\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mToTensor\u001B[49m())\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m train:\n\u001B[0;32m      7\u001B[0m         transforms\u001B[38;5;241m.\u001B[39mappend(T\u001B[38;5;241m.\u001B[39mRandomHorizontalFlip(\u001B[38;5;241m0.5\u001B[39m))\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'transforms' has no attribute 'ToTensor'"
     ]
    }
   ],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}